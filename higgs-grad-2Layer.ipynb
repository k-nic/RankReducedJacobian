{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000, 30)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tdata = pd.read_csv('./higgs/training.csv')\n",
    "nasdaq = np.array(tdata)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=nasdaq[:,1:nasdaq.shape[1]-2]\n",
    "y=nasdaq[:,nasdaq.shape[1]-1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.5, random_state =  14)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# training\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "# validation\n",
    "norm_vcolumns=x_test\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000, 30)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "dimensionality = 30\n",
    "\n",
    "iter_num = 60 # number of iterations of alternating scheme\n",
    "steps_number = 10000 # number of gradient steps\n",
    "\n",
    "code_size1 = 20 #dimension of code1\n",
    "code_size2 = 10 #dimension of code2\n",
    "k = 5 #needed dimension code_size = 2k k=10,20\n",
    "gamma = 1.0 # smoothness of manifold\n",
    "mu = 10.0 # main parameter mu=10,20,40,80,160\n",
    "epsilon = 0.1\n",
    "\n",
    "images = np.reshape(x_train, (-1, dimensionality))\n",
    "print(images.shape)\n",
    "labels = y_train\n",
    "\n",
    "test_images = np.reshape(x_test, (-1, dimensionality))\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "training_data = tf.placeholder(tf.float32, [None, dimensionality])\n",
    "gradient_training_data = tf.placeholder(tf.float32, [None, dimensionality])\n",
    "old_P = tf.placeholder(tf.float32, shape=[None, dimensionality, dimensionality])\n",
    "old_W_1 = tf.placeholder(tf.float32, shape=[dimensionality, code_size1])\n",
    "old_W_2 = tf.placeholder(tf.float32, shape=[code_size1, code_size2])\n",
    "old_W_3 = tf.placeholder(tf.float32, shape=[code_size2, code_size1])\n",
    "old_W_4 = tf.placeholder(tf.float32, shape=[code_size1, dimensionality])\n",
    "old_b_1 = tf.placeholder(tf.float32, shape=[code_size1])\n",
    "old_b_2 = tf.placeholder(tf.float32, shape=[code_size2])\n",
    "old_b_3 = tf.placeholder(tf.float32, shape=[code_size1])\n",
    "old_b_4 = tf.placeholder(tf.float32, shape=[dimensionality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Variables to be tuned\n",
    "W_1 = tf.Variable(tf.truncated_normal([dimensionality, code_size1], stddev=0.1))\n",
    "W_2 = tf.Variable(tf.truncated_normal([code_size1, code_size2], stddev=0.1))\n",
    "W_3 = tf.Variable(tf.truncated_normal([code_size2, code_size1], stddev=0.1))\n",
    "W_4 = tf.Variable(tf.truncated_normal([code_size1, dimensionality], stddev=0.1))\n",
    "b_1 = tf.Variable(tf.constant(0.1, shape=[code_size1]))\n",
    "b_2 = tf.Variable(tf.constant(0.1, shape=[code_size2]))\n",
    "b_3 = tf.Variable(tf.constant(0.1, shape=[code_size1]))\n",
    "b_4 = tf.Variable(tf.constant(0.1, shape=[dimensionality]))\n",
    "\n",
    "\n",
    "code_data1 = tf.nn.sigmoid(tf.matmul(training_data, W_1) + b_1)\n",
    "code_data2 = tf.nn.sigmoid(tf.matmul(code_data1, W_2) + b_2)\n",
    "code_data3 = tf.nn.sigmoid(tf.matmul(code_data2, W_3) + b_3)\n",
    "recover = tf.matmul(code_data3, W_4) + b_4\n",
    "grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        grad_phi_psi.append(tf.gradients(recover[i][j], [training_data[i]], unconnected_gradients='zero')[0])\n",
    "grad_phi_psi = tf.reshape(tf.stack(grad_phi_psi), [batch_size, dimensionality, dimensionality])\n",
    "\n",
    "\n",
    "# this is gradient field close to our points\n",
    "rand_training_data = training_data + tf.random.normal(shape=[batch_size, dimensionality],\n",
    "                                                      mean=0.0,stddev=epsilon)\n",
    "rand_code_data1 = tf.nn.sigmoid(tf.matmul(rand_training_data, W_1) + b_1)\n",
    "rand_code_data2 = tf.nn.sigmoid(tf.matmul(rand_code_data1, W_2) + b_2)\n",
    "rand_code_data3 = tf.nn.sigmoid(tf.matmul(rand_code_data2, W_3) + b_3)\n",
    "rand_recover = tf.matmul(rand_code_data3, W_4) + b_4\n",
    "rand_grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        rand_grad_phi_psi.append(tf.gradients(rand_recover[i][j], [rand_training_data[i]], unconnected_gradients='zero')[0])\n",
    "rand_grad_phi_psi = tf.reshape(tf.stack(rand_grad_phi_psi), [batch_size, dimensionality, dimensionality])\n",
    "\n",
    "new_code_data1 = tf.nn.sigmoid(tf.matmul(gradient_training_data, W_1) + b_1)\n",
    "new_code_data2 = tf.nn.sigmoid(tf.matmul(new_code_data1, W_2) + b_2)\n",
    "new_code_data3 = tf.nn.sigmoid(tf.matmul(new_code_data2, W_3) + b_3)\n",
    "new_recover = tf.matmul(new_code_data3, W_4) + b_4\n",
    "new_grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        new_grad_phi_psi.append(tf.gradients(new_recover[i][j], [gradient_training_data[i]], unconnected_gradients='zero')[0])\n",
    "new_grad_phi_psi = tf.reshape(tf.stack(new_grad_phi_psi), [batch_size, dimensionality, dimensionality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.square(training_data - recover)) + \\\n",
    "       gamma*tf.reduce_mean(tf.square(grad_phi_psi-rand_grad_phi_psi)) + \\\n",
    "       mu*tf.reduce_mean(tf.square(new_grad_phi_psi - old_P))\n",
    "\n",
    "# Training step\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training batch accuracy 98.4511 %\n",
      "Step 1000, training batch accuracy 93.2525 %\n",
      "Step 2000, training batch accuracy 88.5821 %\n",
      "Step 3000, training batch accuracy 61.1664 %\n",
      "Step 4000, training batch accuracy 54.4366 %\n",
      "Step 5000, training batch accuracy 53.732 %\n",
      "Step 6000, training batch accuracy 61.654 %\n",
      "Step 7000, training batch accuracy 71.107 %\n",
      "Step 8000, training batch accuracy 55.6711 %\n",
      "Step 9000, training batch accuracy 47.1964 %\n",
      "Step 0, training batch accuracy 45.1073 %\n",
      "Step 1000, training batch accuracy 54.528 %\n",
      "Step 2000, training batch accuracy 64.4973 %\n",
      "Step 3000, training batch accuracy 50.8303 %\n",
      "Step 4000, training batch accuracy 44.5957 %\n",
      "Step 5000, training batch accuracy 42.893 %\n",
      "Step 6000, training batch accuracy 52.4828 %\n",
      "Step 7000, training batch accuracy 59.2967 %\n",
      "Step 8000, training batch accuracy 46.7084 %\n",
      "Step 9000, training batch accuracy 41.6204 %\n",
      "Step 0, training batch accuracy 41.1509 %\n",
      "Step 1000, training batch accuracy 50.9154 %\n",
      "Step 2000, training batch accuracy 57.3997 %\n",
      "Step 3000, training batch accuracy 45.4814 %\n",
      "Step 4000, training batch accuracy 40.3907 %\n",
      "Step 5000, training batch accuracy 38.5567 %\n",
      "Step 6000, training batch accuracy 46.1895 %\n",
      "Step 7000, training batch accuracy 51.9129 %\n",
      "Step 8000, training batch accuracy 40.4185 %\n",
      "Step 9000, training batch accuracy 36.4674 %\n",
      "Step 0, training batch accuracy 35.409 %\n",
      "Step 1000, training batch accuracy 43.5599 %\n",
      "Step 2000, training batch accuracy 49.5765 %\n",
      "Step 3000, training batch accuracy 39.0304 %\n",
      "Step 4000, training batch accuracy 35.1402 %\n",
      "Step 5000, training batch accuracy 34.1847 %\n",
      "Step 6000, training batch accuracy 41.6163 %\n",
      "Step 7000, training batch accuracy 47.5265 %\n",
      "Step 8000, training batch accuracy 37.6665 %\n",
      "Step 9000, training batch accuracy 33.6402 %\n",
      "Step 0, training batch accuracy 32.7623 %\n",
      "Step 1000, training batch accuracy 39.2493 %\n",
      "Step 2000, training batch accuracy 45.1782 %\n",
      "Step 3000, training batch accuracy 36.0635 %\n",
      "Step 4000, training batch accuracy 32.4783 %\n",
      "Step 5000, training batch accuracy 31.6441 %\n",
      "Step 6000, training batch accuracy 36.8377 %\n",
      "Step 7000, training batch accuracy 42.9509 %\n",
      "Step 8000, training batch accuracy 34.1566 %\n",
      "Step 9000, training batch accuracy 30.948 %\n",
      "Step 0, training batch accuracy 30.2552 %\n",
      "Step 1000, training batch accuracy 34.023 %\n",
      "Step 2000, training batch accuracy 39.7659 %\n",
      "Step 3000, training batch accuracy 31.6198 %\n",
      "Step 4000, training batch accuracy 28.9816 %\n",
      "Step 5000, training batch accuracy 28.4623 %\n",
      "Step 6000, training batch accuracy 30.9059 %\n",
      "Step 7000, training batch accuracy 36.653 %\n",
      "Step 8000, training batch accuracy 29.2705 %\n",
      "Step 9000, training batch accuracy 27.2738 %\n",
      "Step 0, training batch accuracy 26.769 %\n",
      "Step 1000, training batch accuracy 28.8794 %\n",
      "Step 2000, training batch accuracy 34.4237 %\n",
      "Step 3000, training batch accuracy 27.5999 %\n",
      "Step 4000, training batch accuracy 26.0451 %\n",
      "Step 5000, training batch accuracy 25.4587 %\n",
      "Step 6000, training batch accuracy 27.2562 %\n",
      "Step 7000, training batch accuracy 32.7457 %\n",
      "Step 8000, training batch accuracy 26.3829 %\n",
      "Step 9000, training batch accuracy 25.0053 %\n",
      "Step 0, training batch accuracy 24.4025 %\n",
      "Step 1000, training batch accuracy 25.9423 %\n",
      "Step 2000, training batch accuracy 31.3243 %\n",
      "Step 3000, training batch accuracy 25.4139 %\n",
      "Step 4000, training batch accuracy 24.0464 %\n",
      "Step 5000, training batch accuracy 23.5113 %\n",
      "Step 6000, training batch accuracy 24.9058 %\n",
      "Step 7000, training batch accuracy 30.033 %\n",
      "Step 8000, training batch accuracy 24.383 %\n",
      "Step 9000, training batch accuracy 22.9284 %\n",
      "Step 0, training batch accuracy 22.5695 %\n",
      "Step 1000, training batch accuracy 23.6517 %\n",
      "Step 2000, training batch accuracy 28.7654 %\n",
      "Step 3000, training batch accuracy 23.1805 %\n",
      "Step 4000, training batch accuracy 21.7834 %\n",
      "Step 5000, training batch accuracy 21.547 %\n",
      "Step 6000, training batch accuracy 22.1166 %\n",
      "Step 7000, training batch accuracy 27.5077 %\n",
      "Step 8000, training batch accuracy 21.6751 %\n",
      "Step 9000, training batch accuracy 20.6356 %\n",
      "Step 0, training batch accuracy 20.342 %\n",
      "Step 1000, training batch accuracy 20.5735 %\n",
      "Step 2000, training batch accuracy 26.4846 %\n",
      "Step 3000, training batch accuracy 20.3049 %\n",
      "Step 4000, training batch accuracy 19.3521 %\n",
      "Step 5000, training batch accuracy 19.3737 %\n",
      "Step 6000, training batch accuracy 19.453 %\n",
      "Step 7000, training batch accuracy 25.5985 %\n",
      "Step 8000, training batch accuracy 19.3237 %\n",
      "Step 9000, training batch accuracy 18.3317 %\n",
      "Step 0, training batch accuracy 18.6661 %\n",
      "Step 1000, training batch accuracy 18.7899 %\n",
      "Step 2000, training batch accuracy 24.9867 %\n",
      "Step 3000, training batch accuracy 18.8676 %\n",
      "Step 4000, training batch accuracy 17.7013 %\n",
      "Step 5000, training batch accuracy 18.1866 %\n",
      "Step 6000, training batch accuracy 18.3249 %\n",
      "Step 7000, training batch accuracy 24.5259 %\n",
      "Step 8000, training batch accuracy 18.5354 %\n",
      "Step 9000, training batch accuracy 17.2569 %\n",
      "Step 0, training batch accuracy 17.8465 %\n",
      "Step 1000, training batch accuracy 18.007 %\n",
      "Step 2000, training batch accuracy 24.1574 %\n",
      "Step 3000, training batch accuracy 18.2294 %\n",
      "Step 4000, training batch accuracy 16.9426 %\n",
      "Step 5000, training batch accuracy 17.5918 %\n",
      "Step 6000, training batch accuracy 17.7555 %\n",
      "Step 7000, training batch accuracy 23.8527 %\n",
      "Step 8000, training batch accuracy 17.9629 %\n",
      "Step 9000, training batch accuracy 16.6893 %\n",
      "Step 0, training batch accuracy 17.3702 %\n",
      "Step 1000, training batch accuracy 17.5367 %\n",
      "Step 2000, training batch accuracy 23.5882 %\n",
      "Step 3000, training batch accuracy 17.7137 %\n",
      "Step 4000, training batch accuracy 16.4527 %\n",
      "Step 5000, training batch accuracy 17.1501 %\n",
      "Step 6000, training batch accuracy 17.3262 %\n",
      "Step 7000, training batch accuracy 23.3271 %\n",
      "Step 8000, training batch accuracy 17.4596 %\n",
      "Step 9000, training batch accuracy 16.2138 %\n",
      "Step 0, training batch accuracy 16.9141 %\n",
      "Step 1000, training batch accuracy 17.1063 %\n",
      "Step 2000, training batch accuracy 23.0359 %\n",
      "Step 3000, training batch accuracy 17.1929 %\n",
      "Step 4000, training batch accuracy 15.9718 %\n",
      "Step 5000, training batch accuracy 16.6648 %\n",
      "Step 6000, training batch accuracy 16.8732 %\n",
      "Step 7000, training batch accuracy 22.7078 %\n",
      "Step 8000, training batch accuracy 16.9188 %\n",
      "Step 9000, training batch accuracy 15.7371 %\n",
      "Step 0, training batch accuracy 16.4212 %\n",
      "Step 1000, training batch accuracy 16.634 %\n",
      "Step 2000, training batch accuracy 22.3485 %\n",
      "Step 3000, training batch accuracy 16.643 %\n",
      "Step 4000, training batch accuracy 15.5135 %\n",
      "Step 5000, training batch accuracy 16.1971 %\n",
      "Step 6000, training batch accuracy 16.3867 %\n",
      "Step 7000, training batch accuracy 21.9673 %\n",
      "Step 8000, training batch accuracy 16.357 %\n",
      "Step 9000, training batch accuracy 15.2817 %\n",
      "Step 0, training batch accuracy 15.9837 %\n",
      "Step 1000, training batch accuracy 16.1121 %\n",
      "Step 2000, training batch accuracy 21.5894 %\n",
      "Step 3000, training batch accuracy 16.0404 %\n",
      "Step 4000, training batch accuracy 15.0186 %\n",
      "Step 5000, training batch accuracy 15.7648 %\n",
      "Step 6000, training batch accuracy 15.8261 %\n",
      "Step 7000, training batch accuracy 21.2307 %\n",
      "Step 8000, training batch accuracy 15.7069 %\n",
      "Step 9000, training batch accuracy 14.7491 %\n",
      "Step 0, training batch accuracy 15.5622 %\n",
      "Step 1000, training batch accuracy 15.5832 %\n",
      "Step 2000, training batch accuracy 20.9064 %\n",
      "Step 3000, training batch accuracy 15.417 %\n",
      "Step 4000, training batch accuracy 14.5154 %\n",
      "Step 5000, training batch accuracy 15.4031 %\n",
      "Step 6000, training batch accuracy 15.4029 %\n",
      "Step 7000, training batch accuracy 20.6479 %\n",
      "Step 8000, training batch accuracy 15.1969 %\n",
      "Step 9000, training batch accuracy 14.3227 %\n",
      "Step 0, training batch accuracy 15.2756 %\n",
      "Step 1000, training batch accuracy 15.2639 %\n",
      "Step 2000, training batch accuracy 20.4267 %\n",
      "Step 3000, training batch accuracy 15.0199 %\n",
      "Step 4000, training batch accuracy 14.1507 %\n",
      "Step 5000, training batch accuracy 15.1515 %\n",
      "Step 6000, training batch accuracy 15.1343 %\n",
      "Step 7000, training batch accuracy 20.2055 %\n",
      "Step 8000, training batch accuracy 14.8466 %\n",
      "Step 9000, training batch accuracy 13.963 %\n",
      "Step 0, training batch accuracy 14.9854 %\n",
      "Step 1000, training batch accuracy 14.9647 %\n",
      "Step 2000, training batch accuracy 19.931 %\n",
      "Step 3000, training batch accuracy 14.6035 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, training batch accuracy 13.6678 %\n",
      "Step 5000, training batch accuracy 14.6599 %\n",
      "Step 6000, training batch accuracy 14.6278 %\n",
      "Step 7000, training batch accuracy 19.4899 %\n",
      "Step 8000, training batch accuracy 14.1407 %\n",
      "Step 9000, training batch accuracy 13.1406 %\n",
      "Step 0, training batch accuracy 14.1184 %\n",
      "Step 1000, training batch accuracy 14.1438 %\n",
      "Step 2000, training batch accuracy 19.0205 %\n",
      "Step 3000, training batch accuracy 13.6967 %\n",
      "Step 4000, training batch accuracy 12.7205 %\n",
      "Step 5000, training batch accuracy 13.7131 %\n",
      "Step 6000, training batch accuracy 13.7666 %\n",
      "Step 7000, training batch accuracy 18.6917 %\n",
      "Step 8000, training batch accuracy 13.3417 %\n",
      "Step 9000, training batch accuracy 12.3124 %\n",
      "Step 0, training batch accuracy 13.2172 %\n",
      "Step 1000, training batch accuracy 13.1903 %\n",
      "Step 2000, training batch accuracy 18.0597 %\n",
      "Step 3000, training batch accuracy 12.626 %\n",
      "Step 4000, training batch accuracy 11.4829 %\n",
      "Step 5000, training batch accuracy 12.0784 %\n",
      "Step 6000, training batch accuracy 12.0355 %\n",
      "Step 7000, training batch accuracy 16.6846 %\n",
      "Step 8000, training batch accuracy 11.3834 %\n",
      "Step 9000, training batch accuracy 10.6035 %\n",
      "Step 0, training batch accuracy 11.0446 %\n",
      "Step 1000, training batch accuracy 11.3189 %\n",
      "Step 2000, training batch accuracy 15.8056 %\n",
      "Step 3000, training batch accuracy 10.6971 %\n",
      "Step 4000, training batch accuracy 10.2737 %\n",
      "Step 5000, training batch accuracy 10.6485 %\n",
      "Step 6000, training batch accuracy 11.0303 %\n",
      "Step 7000, training batch accuracy 15.4443 %\n",
      "Step 8000, training batch accuracy 10.3396 %\n",
      "Step 9000, training batch accuracy 10.0736 %\n",
      "Step 0, training batch accuracy 10.3816 %\n",
      "Step 1000, training batch accuracy 10.7948 %\n",
      "Step 2000, training batch accuracy 15.1702 %\n",
      "Step 3000, training batch accuracy 10.0553 %\n",
      "Step 4000, training batch accuracy 9.89404 %\n",
      "Step 5000, training batch accuracy 10.1573 %\n",
      "Step 6000, training batch accuracy 10.5858 %\n",
      "Step 7000, training batch accuracy 14.9067 %\n",
      "Step 8000, training batch accuracy 9.82722 %\n",
      "Step 9000, training batch accuracy 9.74597 %\n",
      "Step 0, training batch accuracy 9.98997 %\n",
      "Step 1000, training batch accuracy 10.424 %\n",
      "Step 2000, training batch accuracy 14.667 %\n",
      "Step 3000, training batch accuracy 9.66033 %\n",
      "Step 4000, training batch accuracy 9.63233 %\n",
      "Step 5000, training batch accuracy 9.87221 %\n",
      "Step 6000, training batch accuracy 10.3015 %\n",
      "Step 7000, training batch accuracy 14.463 %\n",
      "Step 8000, training batch accuracy 9.53706 %\n",
      "Step 9000, training batch accuracy 9.5413 %\n",
      "Step 0, training batch accuracy 9.78465 %\n",
      "Step 1000, training batch accuracy 10.2022 %\n",
      "Step 2000, training batch accuracy 14.289 %\n",
      "Step 3000, training batch accuracy 9.44021 %\n",
      "Step 4000, training batch accuracy 9.46395 %\n",
      "Step 5000, training batch accuracy 9.71336 %\n",
      "Step 6000, training batch accuracy 10.1154 %\n",
      "Step 7000, training batch accuracy 14.1357 %\n",
      "Step 8000, training batch accuracy 9.35886 %\n",
      "Step 9000, training batch accuracy 9.39465 %\n",
      "Step 0, training batch accuracy 9.64982 %\n",
      "Step 1000, training batch accuracy 10.0348 %\n",
      "Step 2000, training batch accuracy 13.9971 %\n",
      "Step 3000, training batch accuracy 9.28644 %\n",
      "Step 4000, training batch accuracy 9.32942 %\n",
      "Step 5000, training batch accuracy 9.58824 %\n",
      "Step 6000, training batch accuracy 9.95608 %\n",
      "Step 7000, training batch accuracy 13.8691 %\n",
      "Step 8000, training batch accuracy 9.21876 %\n",
      "Step 9000, training batch accuracy 9.26514 %\n",
      "Step 0, training batch accuracy 9.52391 %\n",
      "Step 1000, training batch accuracy 9.87572 %\n",
      "Step 2000, training batch accuracy 13.7486 %\n",
      "Step 3000, training batch accuracy 9.15281 %\n",
      "Step 4000, training batch accuracy 9.19905 %\n",
      "Step 5000, training batch accuracy 9.45218 %\n",
      "Step 6000, training batch accuracy 9.79054 %\n",
      "Step 7000, training batch accuracy 13.633 %\n",
      "Step 8000, training batch accuracy 9.08623 %\n",
      "Step 9000, training batch accuracy 9.12867 %\n",
      "Step 0, training batch accuracy 9.36784 %\n",
      "Step 1000, training batch accuracy 9.69738 %\n",
      "Step 2000, training batch accuracy 13.5198 %\n",
      "Step 3000, training batch accuracy 9.01709 %\n",
      "Step 4000, training batch accuracy 9.05182 %\n",
      "Step 5000, training batch accuracy 9.26473 %\n",
      "Step 6000, training batch accuracy 9.59316 %\n",
      "Step 7000, training batch accuracy 13.4066 %\n",
      "Step 8000, training batch accuracy 8.94378 %\n",
      "Step 9000, training batch accuracy 8.96707 %\n",
      "Step 0, training batch accuracy 9.13585 %\n",
      "Step 1000, training batch accuracy 9.47554 %\n",
      "Step 2000, training batch accuracy 13.2908 %\n",
      "Step 3000, training batch accuracy 8.86535 %\n",
      "Step 4000, training batch accuracy 8.87468 %\n",
      "Step 5000, training batch accuracy 8.97486 %\n",
      "Step 6000, training batch accuracy 9.34424 %\n",
      "Step 7000, training batch accuracy 13.17 %\n",
      "Step 8000, training batch accuracy 8.78203 %\n",
      "Step 9000, training batch accuracy 8.77801 %\n",
      "Step 0, training batch accuracy 8.78024 %\n",
      "Step 1000, training batch accuracy 9.20334 %\n",
      "Step 2000, training batch accuracy 13.0434 %\n",
      "Step 3000, training batch accuracy 8.69633 %\n",
      "Step 4000, training batch accuracy 8.68418 %\n",
      "Step 5000, training batch accuracy 8.56064 %\n",
      "Step 6000, training batch accuracy 9.06249 %\n",
      "Step 7000, training batch accuracy 12.9126 %\n",
      "Step 8000, training batch accuracy 8.61251 %\n",
      "Step 9000, training batch accuracy 8.60098 %\n",
      "Step 0, training batch accuracy 8.33612 %\n",
      "Step 1000, training batch accuracy 8.93133 %\n",
      "Step 2000, training batch accuracy 12.7822 %\n",
      "Step 3000, training batch accuracy 8.53477 %\n",
      "Step 4000, training batch accuracy 8.53118 %\n",
      "Step 5000, training batch accuracy 8.13123 %\n",
      "Step 6000, training batch accuracy 8.81731 %\n",
      "Step 7000, training batch accuracy 12.6592 %\n",
      "Step 8000, training batch accuracy 8.46639 %\n",
      "Step 9000, training batch accuracy 8.4727 %\n",
      "Step 0, training batch accuracy 7.96134 %\n",
      "Step 1000, training batch accuracy 8.72361 %\n",
      "Step 2000, training batch accuracy 12.5459 %\n",
      "Step 3000, training batch accuracy 8.40642 %\n",
      "Step 4000, training batch accuracy 8.4203 %\n",
      "Step 5000, training batch accuracy 7.82816 %\n",
      "Step 6000, training batch accuracy 8.64655 %\n",
      "Step 7000, training batch accuracy 12.4411 %\n",
      "Step 8000, training batch accuracy 8.35153 %\n",
      "Step 9000, training batch accuracy 8.36919 %\n",
      "Step 0, training batch accuracy 7.72571 %\n",
      "Step 1000, training batch accuracy 8.58138 %\n",
      "Step 2000, training batch accuracy 12.3426 %\n",
      "Step 3000, training batch accuracy 8.29826 %\n",
      "Step 4000, training batch accuracy 8.31653 %\n",
      "Step 5000, training batch accuracy 7.6461 %\n",
      "Step 6000, training batch accuracy 8.52428 %\n",
      "Step 7000, training batch accuracy 12.2483 %\n",
      "Step 8000, training batch accuracy 8.24402 %\n",
      "Step 9000, training batch accuracy 8.261 %\n",
      "Step 0, training batch accuracy 7.58256 %\n",
      "Step 1000, training batch accuracy 8.47247 %\n",
      "Step 2000, training batch accuracy 12.1561 %\n",
      "Step 3000, training batch accuracy 8.18735 %\n",
      "Step 4000, training batch accuracy 8.20216 %\n",
      "Step 5000, training batch accuracy 7.53015 %\n",
      "Step 6000, training batch accuracy 8.42404 %\n",
      "Step 7000, training batch accuracy 12.0646 %\n",
      "Step 8000, training batch accuracy 8.12763 %\n",
      "Step 9000, training batch accuracy 8.13996 %\n",
      "Step 0, training batch accuracy 7.48549 %\n",
      "Step 1000, training batch accuracy 8.37751 %\n",
      "Step 2000, training batch accuracy 11.9725 %\n",
      "Step 3000, training batch accuracy 8.06469 %\n",
      "Step 4000, training batch accuracy 8.07447 %\n",
      "Step 5000, training batch accuracy 7.4463 %\n",
      "Step 6000, training batch accuracy 8.33176 %\n",
      "Step 7000, training batch accuracy 11.8789 %\n",
      "Step 8000, training batch accuracy 7.99848 %\n",
      "Step 9000, training batch accuracy 8.0058 %\n",
      "Step 0, training batch accuracy 7.41101 %\n",
      "Step 1000, training batch accuracy 8.28588 %\n",
      "Step 2000, training batch accuracy 11.7835 %\n",
      "Step 3000, training batch accuracy 7.92905 %\n",
      "Step 4000, training batch accuracy 7.93403 %\n",
      "Step 5000, training batch accuracy 7.37835 %\n",
      "Step 6000, training batch accuracy 8.23933 %\n",
      "Step 7000, training batch accuracy 11.6861 %\n",
      "Step 8000, training batch accuracy 7.85679 %\n",
      "Step 9000, training batch accuracy 7.85933 %\n",
      "Step 0, training batch accuracy 7.3472 %\n",
      "Step 1000, training batch accuracy 8.19199 %\n",
      "Step 2000, training batch accuracy 11.5871 %\n",
      "Step 3000, training batch accuracy 7.78257 %\n",
      "Step 4000, training batch accuracy 7.78195 %\n",
      "Step 5000, training batch accuracy 7.31673 %\n",
      "Step 6000, training batch accuracy 8.14388 %\n",
      "Step 7000, training batch accuracy 11.4869 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000, training batch accuracy 7.7073 %\n",
      "Step 9000, training batch accuracy 7.70218 %\n",
      "Step 0, training batch accuracy 7.28633 %\n",
      "Step 1000, training batch accuracy 8.09498 %\n",
      "Step 2000, training batch accuracy 11.3859 %\n",
      "Step 3000, training batch accuracy 7.6318 %\n",
      "Step 4000, training batch accuracy 7.62036 %\n",
      "Step 5000, training batch accuracy 7.25556 %\n",
      "Step 6000, training batch accuracy 8.04526 %\n",
      "Step 7000, training batch accuracy 11.2845 %\n",
      "Step 8000, training batch accuracy 7.55686 %\n",
      "Step 9000, training batch accuracy 7.53691 %\n",
      "Step 0, training batch accuracy 7.22408 %\n",
      "Step 1000, training batch accuracy 7.99474 %\n",
      "Step 2000, training batch accuracy 11.1834 %\n",
      "Step 3000, training batch accuracy 7.48314 %\n",
      "Step 4000, training batch accuracy 7.45227 %\n",
      "Step 5000, training batch accuracy 7.19168 %\n",
      "Step 6000, training batch accuracy 7.94356 %\n",
      "Step 7000, training batch accuracy 11.0833 %\n",
      "Step 8000, training batch accuracy 7.41114 %\n",
      "Step 9000, training batch accuracy 7.36688 %\n",
      "Step 0, training batch accuracy 7.15825 %\n",
      "Step 1000, training batch accuracy 7.89191 %\n",
      "Step 2000, training batch accuracy 10.9847 %\n",
      "Step 3000, training batch accuracy 7.34122 %\n",
      "Step 4000, training batch accuracy 7.28113 %\n",
      "Step 5000, training batch accuracy 7.12379 %\n",
      "Step 6000, training batch accuracy 7.84006 %\n",
      "Step 7000, training batch accuracy 10.8883 %\n",
      "Step 8000, training batch accuracy 7.27351 %\n",
      "Step 9000, training batch accuracy 7.19537 %\n",
      "Step 0, training batch accuracy 7.08834 %\n",
      "Step 1000, training batch accuracy 7.78831 %\n",
      "Step 2000, training batch accuracy 10.7944 %\n",
      "Step 3000, training batch accuracy 7.20804 %\n",
      "Step 4000, training batch accuracy 7.10982 %\n",
      "Step 5000, training batch accuracy 7.05201 %\n",
      "Step 6000, training batch accuracy 7.73693 %\n",
      "Step 7000, training batch accuracy 10.7036 %\n",
      "Step 8000, training batch accuracy 7.14469 %\n",
      "Step 9000, training batch accuracy 7.02465 %\n",
      "Step 0, training batch accuracy 7.01494 %\n",
      "Step 1000, training batch accuracy 7.68615 %\n",
      "Step 2000, training batch accuracy 10.6159 %\n",
      "Step 3000, training batch accuracy 7.08324 %\n",
      "Step 4000, training batch accuracy 6.93985 %\n",
      "Step 5000, training batch accuracy 6.97716 %\n",
      "Step 6000, training batch accuracy 7.63599 %\n",
      "Step 7000, training batch accuracy 10.5318 %\n",
      "Step 8000, training batch accuracy 7.02347 %\n",
      "Step 9000, training batch accuracy 6.85533 %\n",
      "Step 0, training batch accuracy 6.93865 %\n",
      "Step 1000, training batch accuracy 7.58633 %\n",
      "Step 2000, training batch accuracy 10.4512 %\n",
      "Step 3000, training batch accuracy 6.96508 %\n",
      "Step 4000, training batch accuracy 6.77086 %\n",
      "Step 5000, training batch accuracy 6.89924 %\n",
      "Step 6000, training batch accuracy 7.53684 %\n",
      "Step 7000, training batch accuracy 10.3741 %\n",
      "Step 8000, training batch accuracy 6.90779 %\n",
      "Step 9000, training batch accuracy 6.6862 %\n",
      "Step 0, training batch accuracy 6.8586 %\n",
      "Step 1000, training batch accuracy 7.48699 %\n",
      "Step 2000, training batch accuracy 10.3005 %\n",
      "Step 3000, training batch accuracy 6.8513 %\n",
      "Step 4000, training batch accuracy 6.60105 %\n",
      "Step 5000, training batch accuracy 6.8163 %\n",
      "Step 6000, training batch accuracy 7.43617 %\n",
      "Step 7000, training batch accuracy 10.2303 %\n",
      "Step 8000, training batch accuracy 6.79533 %\n",
      "Step 9000, training batch accuracy 6.5152 %\n",
      "Step 0, training batch accuracy 6.77179 %\n",
      "Step 1000, training batch accuracy 7.38376 %\n",
      "Step 2000, training batch accuracy 10.1634 %\n",
      "Step 3000, training batch accuracy 6.73965 %\n",
      "Step 4000, training batch accuracy 6.4286 %\n",
      "Step 5000, training batch accuracy 6.72454 %\n",
      "Step 6000, training batch accuracy 7.32924 %\n",
      "Step 7000, training batch accuracy 10.0999 %\n",
      "Step 8000, training batch accuracy 6.68411 %\n",
      "Step 9000, training batch accuracy 6.34142 %\n",
      "Step 0, training batch accuracy 6.67409 %\n",
      "Step 1000, training batch accuracy 7.27227 %\n",
      "Step 2000, training batch accuracy 10.0398 %\n",
      "Step 3000, training batch accuracy 6.62875 %\n",
      "Step 4000, training batch accuracy 6.25414 %\n",
      "Step 5000, training batch accuracy 6.62016 %\n",
      "Step 6000, training batch accuracy 7.21276 %\n",
      "Step 7000, training batch accuracy 9.98344 %\n",
      "Step 8000, training batch accuracy 6.57374 %\n",
      "Step 9000, training batch accuracy 6.16756 %\n",
      "Step 0, training batch accuracy 6.56277 %\n",
      "Step 1000, training batch accuracy 7.15093 %\n",
      "Step 2000, training batch accuracy 9.9311 %\n",
      "Step 3000, training batch accuracy 6.5195 %\n",
      "Step 4000, training batch accuracy 6.08279 %\n",
      "Step 5000, training batch accuracy 6.50228 %\n",
      "Step 6000, training batch accuracy 7.08727 %\n",
      "Step 7000, training batch accuracy 9.88304 %\n",
      "Step 8000, training batch accuracy 6.46656 %\n",
      "Step 9000, training batch accuracy 6.00113 %\n",
      "Step 0, training batch accuracy 6.43943 %\n",
      "Step 1000, training batch accuracy 7.02252 %\n",
      "Step 2000, training batch accuracy 9.83945 %\n",
      "Step 3000, training batch accuracy 6.41556 %\n",
      "Step 4000, training batch accuracy 5.92388 %\n",
      "Step 5000, training batch accuracy 6.37529 %\n",
      "Step 6000, training batch accuracy 6.95761 %\n",
      "Step 7000, training batch accuracy 9.80036 %\n",
      "Step 8000, training batch accuracy 6.36711 %\n",
      "Step 9000, training batch accuracy 5.85216 %\n",
      "Step 0, training batch accuracy 6.31117 %\n",
      "Step 1000, training batch accuracy 6.8936 %\n",
      "Step 2000, training batch accuracy 9.76562 %\n",
      "Step 3000, training batch accuracy 6.32171 %\n",
      "Step 4000, training batch accuracy 5.7868 %\n",
      "Step 5000, training batch accuracy 6.24839 %\n",
      "Step 6000, training batch accuracy 6.83147 %\n",
      "Step 7000, training batch accuracy 9.73492 %\n",
      "Step 8000, training batch accuracy 6.27969 %\n",
      "Step 9000, training batch accuracy 5.72818 %\n",
      "Step 0, training batch accuracy 6.18814 %\n",
      "Step 1000, training batch accuracy 6.77202 %\n",
      "Step 2000, training batch accuracy 9.70782 %\n",
      "Step 3000, training batch accuracy 6.24118 %\n",
      "Step 4000, training batch accuracy 5.67631 %\n",
      "Step 5000, training batch accuracy 6.13136 %\n",
      "Step 6000, training batch accuracy 6.7158 %\n",
      "Step 7000, training batch accuracy 9.68385 %\n",
      "Step 8000, training batch accuracy 6.20614 %\n",
      "Step 9000, training batch accuracy 5.63088 %\n",
      "Step 0, training batch accuracy 6.07863 %\n",
      "Step 1000, training batch accuracy 6.6631 %\n",
      "Step 2000, training batch accuracy 9.66244 %\n",
      "Step 3000, training batch accuracy 6.17438 %\n",
      "Step 4000, training batch accuracy 5.59134 %\n",
      "Step 5000, training batch accuracy 6.03027 %\n",
      "Step 6000, training batch accuracy 6.61398 %\n",
      "Step 7000, training batch accuracy 9.64311 %\n",
      "Step 8000, training batch accuracy 6.14562 %\n",
      "Step 9000, training batch accuracy 5.55703 %\n",
      "Step 0, training batch accuracy 5.98632 %\n",
      "Step 1000, training batch accuracy 6.56832 %\n",
      "Step 2000, training batch accuracy 9.62538 %\n",
      "Step 3000, training batch accuracy 6.11956 %\n",
      "Step 4000, training batch accuracy 5.52727 %\n",
      "Step 5000, training batch accuracy 5.94664 %\n",
      "Step 6000, training batch accuracy 6.5259 %\n",
      "Step 7000, training batch accuracy 9.60883 %\n",
      "Step 8000, training batch accuracy 6.09586 %\n",
      "Step 9000, training batch accuracy 5.50138 %\n",
      "Step 0, training batch accuracy 5.91096 %\n",
      "Step 1000, training batch accuracy 6.48644 %\n",
      "Step 2000, training batch accuracy 9.59314 %\n",
      "Step 3000, training batch accuracy 6.0742 %\n",
      "Step 4000, training batch accuracy 5.47876 %\n",
      "Step 5000, training batch accuracy 5.87893 %\n",
      "Step 6000, training batch accuracy 6.44965 %\n",
      "Step 7000, training batch accuracy 9.57803 %\n",
      "Step 8000, training batch accuracy 6.05432 %\n",
      "Step 9000, training batch accuracy 5.45887 %\n",
      "Step 0, training batch accuracy 5.85021 %\n",
      "Step 1000, training batch accuracy 6.41525 %\n",
      "Step 2000, training batch accuracy 9.56326 %\n",
      "Step 3000, training batch accuracy 6.03595 %\n",
      "Step 4000, training batch accuracy 5.44127 %\n",
      "Step 5000, training batch accuracy 5.82442 %\n",
      "Step 6000, training batch accuracy 6.38298 %\n",
      "Step 7000, training batch accuracy 9.5487 %\n",
      "Step 8000, training batch accuracy 6.01888 %\n",
      "Step 9000, training batch accuracy 5.42557 %\n",
      "Step 0, training batch accuracy 5.80124 %\n",
      "Step 1000, training batch accuracy 6.35261 %\n",
      "Step 2000, training batch accuracy 9.53421 %\n",
      "Step 3000, training batch accuracy 6.00293 %\n",
      "Step 4000, training batch accuracy 5.41146 %\n",
      "Step 5000, training batch accuracy 5.78034 %\n",
      "Step 6000, training batch accuracy 6.32394 %\n",
      "Step 7000, training batch accuracy 9.5197 %\n",
      "Step 8000, training batch accuracy 5.98796 %\n",
      "Step 9000, training batch accuracy 5.39867 %\n",
      "Step 0, training batch accuracy 5.76143 %\n",
      "Step 1000, training batch accuracy 6.29682 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, training batch accuracy 9.50512 %\n",
      "Step 3000, training batch accuracy 5.97381 %\n",
      "Step 4000, training batch accuracy 5.387 %\n",
      "Step 5000, training batch accuracy 5.74426 %\n",
      "Step 6000, training batch accuracy 6.27108 %\n",
      "Step 7000, training batch accuracy 9.49041 %\n",
      "Step 8000, training batch accuracy 5.9604 %\n",
      "Step 9000, training batch accuracy 5.37627 %\n",
      "Step 0, training batch accuracy 5.72861 %\n",
      "Step 1000, training batch accuracy 6.24663 %\n",
      "Step 2000, training batch accuracy 9.47557 %\n",
      "Step 3000, training batch accuracy 5.94763 %\n",
      "Step 4000, training batch accuracy 5.36633 %\n",
      "Step 5000, training batch accuracy 5.71429 %\n",
      "Step 6000, training batch accuracy 6.22335 %\n",
      "Step 7000, training batch accuracy 9.46057 %\n",
      "Step 8000, training batch accuracy 5.93541 %\n",
      "Step 9000, training batch accuracy 5.35708 %\n",
      "Step 0, training batch accuracy 5.70113 %\n",
      "Step 1000, training batch accuracy 6.20116 %\n",
      "Step 2000, training batch accuracy 9.44539 %\n",
      "Step 3000, training batch accuracy 5.92369 %\n",
      "Step 4000, training batch accuracy 5.34841 %\n",
      "Step 5000, training batch accuracy 5.68897 %\n",
      "Step 6000, training batch accuracy 6.17998 %\n",
      "Step 7000, training batch accuracy 9.43005 %\n",
      "Step 8000, training batch accuracy 5.9124 %\n",
      "Step 9000, training batch accuracy 5.34025 %\n",
      "Step 0, training batch accuracy 5.67771 %\n",
      "Step 1000, training batch accuracy 6.15978 %\n",
      "Step 2000, training batch accuracy 9.41454 %\n",
      "Step 3000, training batch accuracy 5.90151 %\n",
      "Step 4000, training batch accuracy 5.33252 %\n",
      "Step 5000, training batch accuracy 5.66722 %\n",
      "Step 6000, training batch accuracy 6.14049 %\n",
      "Step 7000, training batch accuracy 9.39887 %\n",
      "Step 8000, training batch accuracy 5.89097 %\n",
      "Step 9000, training batch accuracy 5.32518 %\n",
      "Step 0, training batch accuracy 5.6574 %\n",
      "Step 1000, training batch accuracy 6.12206 %\n",
      "Step 2000, training batch accuracy 9.38306 %\n",
      "Step 3000, training batch accuracy 5.88074 %\n",
      "Step 4000, training batch accuracy 5.31818 %\n",
      "Step 5000, training batch accuracy 5.64819 %\n",
      "Step 6000, training batch accuracy 6.10446 %\n",
      "Step 7000, training batch accuracy 9.36711 %\n",
      "Step 8000, training batch accuracy 5.8708 %\n",
      "Step 9000, training batch accuracy 5.31148 %\n"
     ]
    }
   ],
   "source": [
    "N = 125000\n",
    "x_train = images[:N]\n",
    "N_grad = 1000\n",
    "grad_x_train = images[0:N_grad*125:125]\n",
    "\n",
    "\n",
    "cur_U = np.zeros((N_grad, dimensionality, k))\n",
    "cur_Sigma = np.zeros((N_grad, k, k))\n",
    "cur_V = np.zeros((N_grad, k, dimensionality))\n",
    "feed_P = np.zeros((batch_size, dimensionality, dimensionality))\n",
    "cur_W_1 = np.random.normal(0, 0.35, (dimensionality, code_size1))\n",
    "cur_W_2 = np.random.normal(0, 0.35, (code_size1, code_size2))\n",
    "cur_W_3 = np.random.normal(0, 0.35, (code_size2, code_size1))\n",
    "cur_W_4 = np.random.normal(0, 0.35, (code_size1, dimensionality))\n",
    "cur_b_1 = np.zeros((code_size1))\n",
    "cur_b_2 = np.zeros((code_size2))\n",
    "cur_b_3 = np.zeros((code_size1))\n",
    "cur_b_4 = np.zeros((dimensionality))\n",
    "\n",
    "num_batches = int(N/batch_size)\n",
    "grad_num_batches = int(N_grad/batch_size)\n",
    "\n",
    "for iter in range(iter_num):\n",
    "    for i in range(steps_number):\n",
    "        # Get the next batch\n",
    "        which_batch = i%num_batches\n",
    "        input_batch = x_train[which_batch*batch_size:(which_batch+1)*batch_size]\n",
    "        grad_which_batch = i%grad_num_batches\n",
    "        grad_input_batch = grad_x_train[grad_which_batch*batch_size:(grad_which_batch+1)*batch_size]\n",
    "        for r in range(batch_size):\n",
    "            U = cur_U[grad_which_batch*batch_size+r]\n",
    "            Sigma = cur_Sigma[grad_which_batch*batch_size+r]\n",
    "            V = cur_V[grad_which_batch*batch_size+r]\n",
    "            feed_P[r] = np.matmul(U,np.matmul(Sigma,V))\n",
    "        feed_dict = {training_data: input_batch, gradient_training_data: grad_input_batch, \n",
    "                     old_P:feed_P,\n",
    "                  old_W_1:cur_W_1, old_W_2:cur_W_2, old_W_3:cur_W_3, old_W_4:cur_W_4, \n",
    "                     old_b_1:cur_b_1, old_b_2:cur_b_2, old_b_3:cur_b_3, old_b_4:cur_b_4}\n",
    "        # Run the training step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "        # Print the accuracy progress on the batch every 100 steps\n",
    "        if i%1000 == 0:\n",
    "            train_accuracy = sess.run(loss, feed_dict=feed_dict)\n",
    "            print(\"Step %d, training batch accuracy %g %%\"%(i, train_accuracy*100))\n",
    "    for grad_which_batch in range(grad_num_batches):\n",
    "        grad_input_batch = grad_x_train[grad_which_batch*batch_size:(grad_which_batch+1)*batch_size]\n",
    "        feed_dict = {gradient_training_data: grad_input_batch}\n",
    "        local_grad = sess.run(new_grad_phi_psi, feed_dict=feed_dict)\n",
    "        for r in range(batch_size):\n",
    "            u, s, vh = np.linalg.svd(local_grad[r,:,:], full_matrices=True)\n",
    "            cur_U[grad_which_batch*batch_size+r] = u[:,0:k:1]\n",
    "            cur_V[grad_which_batch*batch_size+r] = np.transpose(vh[:,0:k:1])\n",
    "            cur_Sigma[grad_which_batch*batch_size+r] = np.diag(s[0:k:1])\n",
    "    [cur_W_1, cur_W_2, cur_W_3, cur_W_4, cur_b_1, cur_b_2, cur_b_3, cur_b_4] = sess.run([W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image[0] \tpred: b \torig: s \tacc: 0.0%\n",
      "test image[100] \tpred: s \torig: b \tacc: 74.26%\n",
      "test image[200] \tpred: s \torig: b \tacc: 67.66%\n",
      "test image[300] \tpred: b \torig: b \tacc: 69.1%\n",
      "test image[400] \tpred: b \torig: b \tacc: 68.83%\n",
      "test image[500] \tpred: s \torig: b \tacc: 68.86%\n",
      "test image[600] \tpred: b \torig: s \tacc: 69.38%\n",
      "test image[700] \tpred: b \torig: b \tacc: 69.9%\n",
      "test image[800] \tpred: s \torig: b \tacc: 70.16%\n",
      "test image[900] \tpred: b \torig: b \tacc: 70.81%\n",
      "test image[1000] \tpred: b \torig: b \tacc: 71.33%\n",
      "test image[1100] \tpred: s \torig: s \tacc: 71.39%\n",
      "test image[1200] \tpred: b \torig: s \tacc: 70.69%\n",
      "test image[1300] \tpred: b \torig: b \tacc: 70.95%\n",
      "test image[1400] \tpred: s \torig: b \tacc: 71.23%\n",
      "test image[1500] \tpred: b \torig: s \tacc: 71.35%\n",
      "test image[1600] \tpred: s \torig: s \tacc: 71.52%\n",
      "test image[1700] \tpred: b \torig: b \tacc: 71.31%\n",
      "test image[1800] \tpred: s \torig: s \tacc: 71.57%\n",
      "test image[1900] \tpred: b \torig: b \tacc: 71.8%\n",
      "test image[2000] \tpred: b \torig: s \tacc: 71.61%\n",
      "test image[2100] \tpred: s \torig: b \tacc: 71.82%\n",
      "test image[2200] \tpred: b \torig: b \tacc: 71.69%\n",
      "test image[2300] \tpred: s \torig: s \tacc: 71.71%\n",
      "test image[2400] \tpred: b \torig: b \tacc: 71.93%\n",
      "test image[2500] \tpred: b \torig: b \tacc: 72.13%\n",
      "test image[2600] \tpred: b \torig: b \tacc: 72.4%\n",
      "test image[2700] \tpred: s \torig: s \tacc: 72.38%\n",
      "test image[2800] \tpred: s \torig: b \tacc: 72.33%\n",
      "test image[2900] \tpred: b \torig: b \tacc: 72.63%\n",
      "test image[3000] \tpred: b \torig: b \tacc: 72.81%\n",
      "test image[3100] \tpred: b \torig: b \tacc: 73.2%\n",
      "test image[3200] \tpred: b \torig: s \tacc: 73.16%\n",
      "test image[3300] \tpred: b \torig: b \tacc: 72.83%\n",
      "test image[3400] \tpred: b \torig: s \tacc: 72.95%\n",
      "test image[3500] \tpred: b \torig: b \tacc: 72.92%\n",
      "test image[3600] \tpred: b \torig: b \tacc: 72.84%\n",
      "test image[3700] \tpred: b \torig: s \tacc: 72.79%\n",
      "test image[3800] \tpred: b \torig: s \tacc: 72.72%\n",
      "test image[3900] \tpred: s \torig: b \tacc: 72.75%\n",
      "test image[4000] \tpred: s \torig: s \tacc: 72.63%\n",
      "test image[4100] \tpred: b \torig: s \tacc: 72.93%\n",
      "test image[4200] \tpred: b \torig: b \tacc: 72.98%\n",
      "test image[4300] \tpred: b \torig: s \tacc: 72.84%\n",
      "test image[4400] \tpred: b \torig: b \tacc: 72.78%\n",
      "test image[4500] \tpred: b \torig: b \tacc: 72.89%\n",
      "test image[4600] \tpred: b \torig: b \tacc: 72.83%\n",
      "test image[4700] \tpred: b \torig: s \tacc: 72.96%\n",
      "test image[4800] \tpred: s \torig: s \tacc: 72.98%\n",
      "test image[4900] \tpred: b \torig: b \tacc: 72.94%\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(img_a, img_b):\n",
    "    '''Finds the distance between 2 images: img_a, img_b'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((img_a - img_b) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "train_images = np.asarray(images[:5000])\n",
    "train_labels = np.asarray(labels[:5000])\n",
    "test_images = np.asarray(test_images[:5000])\n",
    "test_labels = np.asarray(test_labels[:5000])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. /(1+np.exp(-x))\n",
    "def new_euclidean_distance(img_a, img_b):\n",
    "    img_a = np.reshape(img_a, (1,-1))\n",
    "    img_b = np.reshape(img_b, (1,-1))\n",
    "    img_a = sigmoid(np.matmul(img_a, cur_W_1) + cur_b_1)\n",
    "    img_a = sigmoid(np.matmul(img_a, cur_W_2) + cur_b_2)\n",
    "    img_b = sigmoid(np.matmul(img_b, cur_W_1) + cur_b_1)\n",
    "    img_b = sigmoid(np.matmul(img_b, cur_W_2) + cur_b_2)\n",
    "    return np.sum((img_a - img_b) ** 2)\n",
    "\n",
    "def new_predict(k, train_images, train_labels, test_images):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(new_euclidean_distance(test_image, image), label)\n",
    "                    for (image, label) in zip(train_images, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "# Predicting and printing the accuracy\n",
    "i = 0\n",
    "total_correct = 0\n",
    "for test_image in test_images[:5000]:\n",
    "    pred = new_predict(10, train_images, train_labels, test_image)\n",
    "    if pred == test_labels[i]:\n",
    "        total_correct += 1\n",
    "    acc = (total_correct / (i+1)) * 100\n",
    "    if i%100 == 0:\n",
    "        print('test image['+str(i)+']', '\\tpred:', pred, '\\torig:', test_labels[i], '\\tacc:', str(round(acc, 2))+'%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
