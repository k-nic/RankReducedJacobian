{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "# Read data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "learning_rate = 0.0001\n",
    "batch_size = 20 # no need to change\n",
    "dimensionality = image_size*image_size\n",
    "#Lambda = 0.0 \n",
    "\n",
    "iter_num = 20 # number of iterations of alternating scheme\n",
    "steps_number = 1000 # number of gradient steps\n",
    "\n",
    "code_size1 = int(dimensionality/2) #dimension of code1\n",
    "code_size2 = 40 #dimension of code2\n",
    "k = 15 #needed dimension code_size = 2k k=10,20\n",
    "gamma = 1.0 # smoothness of manifold\n",
    "mu = 10.0 # main parameter mu=10,20,40,80,160\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "training_data = tf.placeholder(tf.float32, [None, dimensionality])\n",
    "gradient_training_data = tf.placeholder(tf.float32, [None, dimensionality])\n",
    "old_P = tf.placeholder(tf.float32, shape=[None, dimensionality, dimensionality])\n",
    "old_W_1 = tf.placeholder(tf.float32, shape=[dimensionality, code_size1])\n",
    "old_W_2 = tf.placeholder(tf.float32, shape=[code_size1, code_size2])\n",
    "old_W_3 = tf.placeholder(tf.float32, shape=[code_size2, code_size1])\n",
    "old_W_4 = tf.placeholder(tf.float32, shape=[code_size1, dimensionality])\n",
    "old_b_1 = tf.placeholder(tf.float32, shape=[code_size1])\n",
    "old_b_2 = tf.placeholder(tf.float32, shape=[code_size2])\n",
    "old_b_3 = tf.placeholder(tf.float32, shape=[code_size1])\n",
    "old_b_4 = tf.placeholder(tf.float32, shape=[dimensionality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Variables to be tuned\n",
    "W_1 = tf.Variable(tf.truncated_normal([dimensionality, code_size1], stddev=0.1))\n",
    "W_2 = tf.Variable(tf.truncated_normal([code_size1, code_size2], stddev=0.1))\n",
    "W_3 = tf.Variable(tf.truncated_normal([code_size2, code_size1], stddev=0.1))\n",
    "W_4 = tf.Variable(tf.truncated_normal([code_size1, dimensionality], stddev=0.1))\n",
    "b_1 = tf.Variable(tf.constant(0.1, shape=[code_size1]))\n",
    "b_2 = tf.Variable(tf.constant(0.1, shape=[code_size2]))\n",
    "b_3 = tf.Variable(tf.constant(0.1, shape=[code_size1]))\n",
    "b_4 = tf.Variable(tf.constant(0.1, shape=[dimensionality]))\n",
    "\n",
    "\n",
    "code_data1 = tf.nn.sigmoid(tf.matmul(training_data, W_1) + b_1)\n",
    "code_data2 = tf.nn.sigmoid(tf.matmul(code_data1, W_2) + b_2)\n",
    "code_data3 = tf.nn.sigmoid(tf.matmul(code_data2, W_3) + b_3)\n",
    "recover = tf.matmul(code_data3, W_4) + b_4\n",
    "grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        grad_phi_psi.append(tf.gradients(recover[i][j], [training_data[i]], unconnected_gradients='zero')[0])\n",
    "grad_phi_psi = tf.reshape(tf.stack(grad_phi_psi), [batch_size, dimensionality, dimensionality])\n",
    "\n",
    "\n",
    "# this is gradient field close to our points\n",
    "rand_training_data = training_data + tf.random.normal(shape=[batch_size, dimensionality],\n",
    "                                                      mean=0.0,stddev=epsilon)\n",
    "rand_code_data1 = tf.nn.sigmoid(tf.matmul(rand_training_data, W_1) + b_1)\n",
    "rand_code_data2 = tf.nn.sigmoid(tf.matmul(rand_code_data1, W_2) + b_2)\n",
    "rand_code_data3 = tf.nn.sigmoid(tf.matmul(rand_code_data2, W_3) + b_3)\n",
    "rand_recover = tf.matmul(rand_code_data3, W_4) + b_4\n",
    "rand_grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        rand_grad_phi_psi.append(tf.gradients(rand_recover[i][j], [rand_training_data[i]], unconnected_gradients='zero')[0])\n",
    "rand_grad_phi_psi = tf.reshape(tf.stack(rand_grad_phi_psi), [batch_size, dimensionality, dimensionality])\n",
    "\n",
    "\n",
    "new_code_data1 = tf.nn.sigmoid(tf.matmul(gradient_training_data, W_1) + b_1)\n",
    "new_code_data2 = tf.nn.sigmoid(tf.matmul(new_code_data1, W_2) + b_2)\n",
    "new_code_data3 = tf.nn.sigmoid(tf.matmul(new_code_data2, W_3) + b_3)\n",
    "new_recover = tf.matmul(new_code_data3, W_4) + b_4\n",
    "new_grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        new_grad_phi_psi.append(tf.gradients(new_recover[i][j], [gradient_training_data[i]], unconnected_gradients='zero')[0])\n",
    "new_grad_phi_psi = tf.reshape(tf.stack(new_grad_phi_psi), [batch_size, dimensionality, dimensionality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.square(training_data - recover)) + \\\n",
    "       gamma*tf.reduce_mean(tf.square(grad_phi_psi-rand_grad_phi_psi)) + \\\n",
    "       mu*tf.reduce_mean(tf.square(new_grad_phi_psi - old_P))\n",
    "\n",
    "#+ Lambda*tf.reduce_mean(tf.square(grad_phi_psi))\n",
    "\n",
    "# Training step\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training batch accuracy 96.6128 %\n",
      "Step 100, training batch accuracy 8.92065 %\n",
      "Step 200, training batch accuracy 7.19192 %\n",
      "Step 300, training batch accuracy 7.30197 %\n",
      "Step 400, training batch accuracy 7.24543 %\n",
      "Step 500, training batch accuracy 6.54435 %\n",
      "Step 600, training batch accuracy 6.73401 %\n",
      "Step 700, training batch accuracy 6.27144 %\n",
      "Step 800, training batch accuracy 6.07558 %\n",
      "Step 900, training batch accuracy 6.85442 %\n",
      "Step 0, training batch accuracy 6.92658 %\n",
      "Step 100, training batch accuracy 6.69377 %\n",
      "Step 200, training batch accuracy 6.57655 %\n",
      "Step 300, training batch accuracy 6.56761 %\n",
      "Step 400, training batch accuracy 6.48239 %\n",
      "Step 500, training batch accuracy 5.86138 %\n",
      "Step 600, training batch accuracy 5.89766 %\n",
      "Step 700, training batch accuracy 5.33496 %\n",
      "Step 800, training batch accuracy 5.30973 %\n",
      "Step 900, training batch accuracy 5.86441 %\n",
      "Step 0, training batch accuracy 5.47268 %\n",
      "Step 100, training batch accuracy 5.55632 %\n",
      "Step 200, training batch accuracy 5.4157 %\n",
      "Step 300, training batch accuracy 5.15231 %\n",
      "Step 400, training batch accuracy 5.16653 %\n",
      "Step 500, training batch accuracy 4.83173 %\n",
      "Step 600, training batch accuracy 4.83224 %\n",
      "Step 700, training batch accuracy 4.37875 %\n",
      "Step 800, training batch accuracy 4.46196 %\n",
      "Step 900, training batch accuracy 4.88839 %\n",
      "Step 0, training batch accuracy 4.33882 %\n",
      "Step 100, training batch accuracy 4.65462 %\n",
      "Step 200, training batch accuracy 4.51928 %\n",
      "Step 300, training batch accuracy 4.13544 %\n",
      "Step 400, training batch accuracy 4.26071 %\n",
      "Step 500, training batch accuracy 4.13726 %\n",
      "Step 600, training batch accuracy 4.11998 %\n",
      "Step 700, training batch accuracy 3.77455 %\n",
      "Step 800, training batch accuracy 3.88683 %\n",
      "Step 900, training batch accuracy 4.2351 %\n",
      "Step 0, training batch accuracy 3.70772 %\n",
      "Step 100, training batch accuracy 4.0343 %\n",
      "Step 200, training batch accuracy 3.95979 %\n",
      "Step 300, training batch accuracy 3.49712 %\n",
      "Step 400, training batch accuracy 3.72197 %\n",
      "Step 500, training batch accuracy 3.68072 %\n",
      "Step 600, training batch accuracy 3.6514 %\n",
      "Step 700, training batch accuracy 3.38289 %\n",
      "Step 800, training batch accuracy 3.47412 %\n",
      "Step 900, training batch accuracy 3.75836 %\n",
      "Step 0, training batch accuracy 3.31229 %\n",
      "Step 100, training batch accuracy 3.58499 %\n",
      "Step 200, training batch accuracy 3.57144 %\n",
      "Step 300, training batch accuracy 3.0509 %\n",
      "Step 400, training batch accuracy 3.34169 %\n",
      "Step 500, training batch accuracy 3.30526 %\n",
      "Step 600, training batch accuracy 3.27711 %\n",
      "Step 700, training batch accuracy 3.08626 %\n",
      "Step 800, training batch accuracy 3.15087 %\n",
      "Step 900, training batch accuracy 3.37238 %\n",
      "Step 0, training batch accuracy 3.00568 %\n",
      "Step 100, training batch accuracy 3.25711 %\n",
      "Step 200, training batch accuracy 3.25937 %\n",
      "Step 300, training batch accuracy 2.73003 %\n",
      "Step 400, training batch accuracy 3.075 %\n",
      "Step 500, training batch accuracy 3.01315 %\n",
      "Step 600, training batch accuracy 2.97292 %\n",
      "Step 700, training batch accuracy 2.83686 %\n",
      "Step 800, training batch accuracy 2.89987 %\n",
      "Step 900, training batch accuracy 3.06969 %\n",
      "Step 0, training batch accuracy 2.77051 %\n",
      "Step 100, training batch accuracy 3.01318 %\n",
      "Step 200, training batch accuracy 3.01663 %\n",
      "Step 300, training batch accuracy 2.50049 %\n",
      "Step 400, training batch accuracy 2.87699 %\n",
      "Step 500, training batch accuracy 2.79924 %\n",
      "Step 600, training batch accuracy 2.73668 %\n",
      "Step 700, training batch accuracy 2.6185 %\n",
      "Step 800, training batch accuracy 2.70018 %\n",
      "Step 900, training batch accuracy 2.83473 %\n",
      "Step 0, training batch accuracy 2.58319 %\n",
      "Step 100, training batch accuracy 2.8198 %\n",
      "Step 200, training batch accuracy 2.82316 %\n",
      "Step 300, training batch accuracy 2.32476 %\n",
      "Step 400, training batch accuracy 2.7167 %\n",
      "Step 500, training batch accuracy 2.64054 %\n",
      "Step 600, training batch accuracy 2.55729 %\n",
      "Step 700, training batch accuracy 2.44033 %\n",
      "Step 800, training batch accuracy 2.53766 %\n",
      "Step 900, training batch accuracy 2.65117 %\n",
      "Step 0, training batch accuracy 2.42654 %\n",
      "Step 100, training batch accuracy 2.65871 %\n",
      "Step 200, training batch accuracy 2.66443 %\n",
      "Step 300, training batch accuracy 2.18457 %\n",
      "Step 400, training batch accuracy 2.58309 %\n",
      "Step 500, training batch accuracy 2.51696 %\n",
      "Step 600, training batch accuracy 2.41664 %\n",
      "Step 700, training batch accuracy 2.30094 %\n",
      "Step 800, training batch accuracy 2.39972 %\n",
      "Step 900, training batch accuracy 2.50095 %\n",
      "Step 0, training batch accuracy 2.29268 %\n",
      "Step 100, training batch accuracy 2.52026 %\n",
      "Step 200, training batch accuracy 2.53194 %\n",
      "Step 300, training batch accuracy 2.07319 %\n",
      "Step 400, training batch accuracy 2.46972 %\n",
      "Step 500, training batch accuracy 2.41453 %\n",
      "Step 600, training batch accuracy 2.29997 %\n",
      "Step 700, training batch accuracy 2.18929 %\n",
      "Step 800, training batch accuracy 2.2772 %\n",
      "Step 900, training batch accuracy 2.37331 %\n",
      "Step 0, training batch accuracy 2.17726 %\n",
      "Step 100, training batch accuracy 2.40003 %\n",
      "Step 200, training batch accuracy 2.41813 %\n",
      "Step 300, training batch accuracy 1.9844 %\n",
      "Step 400, training batch accuracy 2.37046 %\n",
      "Step 500, training batch accuracy 2.324 %\n",
      "Step 600, training batch accuracy 2.19927 %\n",
      "Step 700, training batch accuracy 2.09509 %\n",
      "Step 800, training batch accuracy 2.16767 %\n",
      "Step 900, training batch accuracy 2.26385 %\n",
      "Step 0, training batch accuracy 2.07709 %\n",
      "Step 100, training batch accuracy 2.29412 %\n",
      "Step 200, training batch accuracy 2.3179 %\n",
      "Step 300, training batch accuracy 1.91096 %\n",
      "Step 400, training batch accuracy 2.28142 %\n",
      "Step 500, training batch accuracy 2.23988 %\n",
      "Step 600, training batch accuracy 2.11067 %\n",
      "Step 700, training batch accuracy 2.01356 %\n",
      "Step 800, training batch accuracy 2.07228 %\n",
      "Step 900, training batch accuracy 2.16967 %\n",
      "Step 0, training batch accuracy 1.99011 %\n",
      "Step 100, training batch accuracy 2.20081 %\n",
      "Step 200, training batch accuracy 2.22933 %\n",
      "Step 300, training batch accuracy 1.848 %\n",
      "Step 400, training batch accuracy 2.20157 %\n",
      "Step 500, training batch accuracy 2.16036 %\n",
      "Step 600, training batch accuracy 2.0322 %\n",
      "Step 700, training batch accuracy 1.94352 %\n",
      "Step 800, training batch accuracy 1.9916 %\n",
      "Step 900, training batch accuracy 2.08833 %\n",
      "Step 0, training batch accuracy 1.9155 %\n",
      "Step 100, training batch accuracy 2.11939 %\n",
      "Step 200, training batch accuracy 2.15199 %\n",
      "Step 300, training batch accuracy 1.79268 %\n",
      "Step 400, training batch accuracy 2.13045 %\n",
      "Step 500, training batch accuracy 2.08602 %\n",
      "Step 600, training batch accuracy 1.96222 %\n",
      "Step 700, training batch accuracy 1.88373 %\n",
      "Step 800, training batch accuracy 1.92394 %\n",
      "Step 900, training batch accuracy 2.01795 %\n",
      "Step 0, training batch accuracy 1.85212 %\n",
      "Step 100, training batch accuracy 2.04823 %\n",
      "Step 200, training batch accuracy 2.08454 %\n",
      "Step 300, training batch accuracy 1.74302 %\n",
      "Step 400, training batch accuracy 2.06696 %\n",
      "Step 500, training batch accuracy 2.01796 %\n",
      "Step 600, training batch accuracy 1.8988 %\n",
      "Step 700, training batch accuracy 1.83192 %\n",
      "Step 800, training batch accuracy 1.86632 %\n",
      "Step 900, training batch accuracy 1.95703 %\n",
      "Step 0, training batch accuracy 1.79782 %\n",
      "Step 100, training batch accuracy 1.98513 %\n",
      "Step 200, training batch accuracy 2.02467 %\n",
      "Step 300, training batch accuracy 1.69775 %\n",
      "Step 400, training batch accuracy 2.00999 %\n",
      "Step 500, training batch accuracy 1.95671 %\n",
      "Step 600, training batch accuracy 1.84039 %\n",
      "Step 700, training batch accuracy 1.78574 %\n",
      "Step 800, training batch accuracy 1.81597 %\n",
      "Step 900, training batch accuracy 1.90418 %\n",
      "Step 0, training batch accuracy 1.75044 %\n",
      "Step 100, training batch accuracy 1.92849 %\n",
      "Step 200, training batch accuracy 1.97036 %\n",
      "Step 300, training batch accuracy 1.65629 %\n",
      "Step 400, training batch accuracy 1.95902 %\n",
      "Step 500, training batch accuracy 1.90215 %\n",
      "Step 600, training batch accuracy 1.78618 %\n",
      "Step 700, training batch accuracy 1.74365 %\n",
      "Step 800, training batch accuracy 1.77111 %\n",
      "Step 900, training batch accuracy 1.85818 %\n",
      "Step 0, training batch accuracy 1.7085 %\n",
      "Step 100, training batch accuracy 1.87731 %\n",
      "Step 200, training batch accuracy 1.92042 %\n",
      "Step 300, training batch accuracy 1.6186 %\n",
      "Step 400, training batch accuracy 1.91384 %\n",
      "Step 500, training batch accuracy 1.85377 %\n",
      "Step 600, training batch accuracy 1.73608 %\n",
      "Step 700, training batch accuracy 1.70489 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800, training batch accuracy 1.73073 %\n",
      "Step 900, training batch accuracy 1.81805 %\n",
      "Step 0, training batch accuracy 1.67117 %\n",
      "Step 100, training batch accuracy 1.83107 %\n",
      "Step 200, training batch accuracy 1.87452 %\n",
      "Step 300, training batch accuracy 1.58478 %\n",
      "Step 400, training batch accuracy 1.87427 %\n",
      "Step 500, training batch accuracy 1.81084 %\n",
      "Step 600, training batch accuracy 1.69048 %\n",
      "Step 700, training batch accuracy 1.66925 %\n",
      "Step 800, training batch accuracy 1.69431 %\n",
      "Step 900, training batch accuracy 1.78307 %\n"
     ]
    }
   ],
   "source": [
    "N = 50000 # x_train.shape[0]=55000\n",
    "x_train = mnist.train.images[:N]\n",
    "N_grad = 1000\n",
    "grad_x_train = mnist.train.images[0:50*N_grad:50]\n",
    "\n",
    "\n",
    "cur_U = np.zeros((N_grad, dimensionality, k))\n",
    "cur_Sigma = np.zeros((N_grad, k, k))\n",
    "cur_V = np.zeros((N_grad, k, dimensionality))\n",
    "feed_P = np.zeros((batch_size, dimensionality, dimensionality))\n",
    "cur_W_1 = np.random.normal(0, 0.35, (dimensionality, code_size1))\n",
    "cur_W_2 = np.random.normal(0, 0.35, (code_size1, code_size2))\n",
    "cur_W_3 = np.random.normal(0, 0.35, (code_size2, code_size1))\n",
    "cur_W_4 = np.random.normal(0, 0.35, (code_size1, dimensionality))\n",
    "cur_b_1 = np.zeros((code_size1))\n",
    "cur_b_2 = np.zeros((code_size2))\n",
    "cur_b_3 = np.zeros((code_size1))\n",
    "cur_b_4 = np.zeros((dimensionality))\n",
    "\n",
    "num_batches = int(N/batch_size)\n",
    "grad_num_batches = int(N_grad/batch_size)\n",
    "\n",
    "for iter in range(iter_num):\n",
    "    for i in range(steps_number):\n",
    "        # Get the next batch\n",
    "        which_batch = i%num_batches\n",
    "        input_batch = x_train[which_batch*batch_size:(which_batch+1)*batch_size]\n",
    "        grad_which_batch = i%grad_num_batches\n",
    "        grad_input_batch = grad_x_train[grad_which_batch*batch_size:(grad_which_batch+1)*batch_size]\n",
    "        for r in range(batch_size):\n",
    "            U = cur_U[grad_which_batch*batch_size+r]\n",
    "            Sigma = cur_Sigma[grad_which_batch*batch_size+r]\n",
    "            V = cur_V[grad_which_batch*batch_size+r]\n",
    "            feed_P[r] = np.matmul(U,np.matmul(Sigma,V))\n",
    "        feed_dict = {training_data: input_batch, gradient_training_data: grad_input_batch, \n",
    "                     old_P:feed_P,\n",
    "                  old_W_1:cur_W_1, old_W_2:cur_W_2, old_W_3:cur_W_3, old_W_4:cur_W_4, \n",
    "                     old_b_1:cur_b_1, old_b_2:cur_b_2, old_b_3:cur_b_3, old_b_4:cur_b_4}\n",
    "        # Run the training step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "        # Print the accuracy progress on the batch every 100 steps\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = sess.run(loss, feed_dict=feed_dict)\n",
    "            print(\"Step %d, training batch accuracy %g %%\"%(i, train_accuracy*100))\n",
    "    for grad_which_batch in range(grad_num_batches):\n",
    "        grad_input_batch = grad_x_train[grad_which_batch*batch_size:(grad_which_batch+1)*batch_size]\n",
    "        feed_dict = {gradient_training_data: grad_input_batch}\n",
    "        local_grad = sess.run(new_grad_phi_psi, feed_dict=feed_dict)\n",
    "        for r in range(batch_size):\n",
    "            u, s, vh = np.linalg.svd(local_grad[r,:,:], full_matrices=True)\n",
    "            cur_U[grad_which_batch*batch_size+r] = u[:,0:k:1]\n",
    "            cur_V[grad_which_batch*batch_size+r] = np.transpose(vh[:,0:k:1])\n",
    "            cur_Sigma[grad_which_batch*batch_size+r] = np.diag(s[0:k:1])\n",
    "    [cur_W_1, cur_W_2, cur_W_3, cur_W_4, cur_b_1, cur_b_2, cur_b_3, cur_b_4] = sess.run([W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image[0] \tpred: 7 \torig: 7 \tacc: 100.0%\n",
      "test image[100] \tpred: 6 \torig: 6 \tacc: 94.06%\n",
      "test image[200] \tpred: 3 \torig: 3 \tacc: 93.53%\n",
      "test image[300] \tpred: 1 \torig: 4 \tacc: 93.02%\n",
      "test image[400] \tpred: 2 \torig: 2 \tacc: 92.02%\n",
      "test image[500] \tpred: 3 \torig: 3 \tacc: 91.82%\n",
      "test image[600] \tpred: 6 \torig: 6 \tacc: 91.51%\n",
      "test image[700] \tpred: 1 \torig: 1 \tacc: 91.73%\n",
      "test image[800] \tpred: 8 \torig: 8 \tacc: 91.89%\n",
      "test image[900] \tpred: 1 \torig: 1 \tacc: 92.12%\n",
      "test image[1000] \tpred: 9 \torig: 9 \tacc: 91.71%\n",
      "test image[1100] \tpred: 7 \torig: 7 \tacc: 91.73%\n",
      "test image[1200] \tpred: 5 \torig: 8 \tacc: 91.51%\n",
      "test image[1300] \tpred: 4 \torig: 4 \tacc: 91.24%\n",
      "test image[1400] \tpred: 6 \torig: 6 \tacc: 91.22%\n",
      "test image[1500] \tpred: 1 \torig: 7 \tacc: 90.94%\n",
      "test image[1600] \tpred: 3 \torig: 3 \tacc: 91.01%\n",
      "test image[1700] \tpred: 0 \torig: 0 \tacc: 91.24%\n",
      "test image[1800] \tpred: 6 \torig: 6 \tacc: 91.0%\n",
      "test image[1900] \tpred: 1 \torig: 1 \tacc: 91.06%\n",
      "test image[2000] \tpred: 6 \torig: 6 \tacc: 91.3%\n",
      "test image[2100] \tpred: 5 \torig: 5 \tacc: 91.1%\n",
      "test image[2200] \tpred: 2 \torig: 2 \tacc: 90.73%\n",
      "test image[2300] \tpred: 3 \torig: 3 \tacc: 90.96%\n",
      "test image[2400] \tpred: 5 \torig: 5 \tacc: 90.84%\n",
      "test image[2500] \tpred: 2 \torig: 2 \tacc: 90.88%\n",
      "test image[2600] \tpred: 8 \torig: 8 \tacc: 90.93%\n",
      "test image[2700] \tpred: 7 \torig: 7 \tacc: 91.0%\n",
      "test image[2800] \tpred: 8 \torig: 8 \tacc: 91.18%\n",
      "test image[2900] \tpred: 9 \torig: 4 \tacc: 91.28%\n",
      "test image[3000] \tpred: 6 \torig: 6 \tacc: 91.37%\n",
      "test image[3100] \tpred: 3 \torig: 5 \tacc: 91.42%\n",
      "test image[3200] \tpred: 9 \torig: 9 \tacc: 91.47%\n",
      "test image[3300] \tpred: 3 \torig: 3 \tacc: 91.4%\n",
      "test image[3400] \tpred: 7 \torig: 7 \tacc: 91.41%\n",
      "test image[3500] \tpred: 4 \torig: 4 \tacc: 91.4%\n",
      "test image[3600] \tpred: 1 \torig: 2 \tacc: 91.34%\n",
      "test image[3700] \tpred: 4 \torig: 4 \tacc: 91.41%\n",
      "test image[3800] \tpred: 6 \torig: 6 \tacc: 91.21%\n",
      "test image[3900] \tpred: 1 \torig: 1 \tacc: 91.18%\n",
      "test image[4000] \tpred: 4 \torig: 9 \tacc: 91.13%\n",
      "test image[4100] \tpred: 2 \torig: 2 \tacc: 91.25%\n",
      "test image[4200] \tpred: 7 \torig: 7 \tacc: 91.29%\n",
      "test image[4300] \tpred: 5 \torig: 5 \tacc: 91.3%\n",
      "test image[4400] \tpred: 9 \torig: 7 \tacc: 91.25%\n",
      "test image[4500] \tpred: 1 \torig: 9 \tacc: 91.22%\n",
      "test image[4600] \tpred: 3 \torig: 3 \tacc: 91.31%\n",
      "test image[4700] \tpred: 9 \torig: 9 \tacc: 91.34%\n",
      "test image[4800] \tpred: 7 \torig: 7 \tacc: 91.36%\n",
      "test image[4900] \tpred: 7 \torig: 7 \tacc: 91.27%\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(img_a, img_b):\n",
    "    '''Finds the distance between 2 images: img_a, img_b'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((img_a - img_b) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "train_images = np.asarray(mnist.train.images[:5000])\n",
    "train_labels = np.asarray(mnist.train.labels[:5000])\n",
    "test_images = np.asarray(mnist.test.images)\n",
    "test_labels = np.asarray(mnist.test.labels)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. /(1+np.exp(-x))\n",
    "def new_euclidean_distance(img_a, img_b):\n",
    "    img_a = np.reshape(img_a, (1,-1))\n",
    "    img_b = np.reshape(img_b, (1,-1))\n",
    "    img_a = sigmoid(np.matmul(img_a, cur_W_1) + cur_b_1)\n",
    "    img_a = sigmoid(np.matmul(img_a, cur_W_2) + cur_b_2)\n",
    "    img_b = sigmoid(np.matmul(img_b, cur_W_1) + cur_b_1)\n",
    "    img_b = sigmoid(np.matmul(img_b, cur_W_2) + cur_b_2)\n",
    "    return np.sum((img_a - img_b) ** 2)\n",
    "\n",
    "def new_predict(k, train_images, train_labels, test_images):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(new_euclidean_distance(test_image, image), label)\n",
    "                    for (image, label) in zip(train_images, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "# Predicting and printing the accuracy\n",
    "i = 0\n",
    "total_correct = 0\n",
    "for test_image in test_images[:5000]:\n",
    "    pred = new_predict(10, train_images, train_labels, test_image)\n",
    "    if pred == test_labels[i]:\n",
    "        total_correct += 1\n",
    "    acc = (total_correct / (i+1)) * 100\n",
    "    if i%100 == 0:\n",
    "        print('test image['+str(i)+']', '\\tpred:', pred, '\\torig:', test_labels[i], '\\tacc:', str(round(acc, 2))+'%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
