{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290506, 54)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"covtype.csv\", sep=\",\")\n",
    "data.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=data[data.columns[:data.shape[1]-1]]\n",
    "y=data[data.columns[data.shape[1]-1:]]-1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.5, random_state =  14)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:5804: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = expressions.where(mask, this, that)\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:5804: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = expressions.where(mask, this, that)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# training\n",
    "norm_tcolumns=x_train[x_train.columns[:10]] # only the first ten columns need normalization, the rest is binary\n",
    "scaler = StandardScaler().fit(norm_tcolumns.values)\n",
    "scaledf = scaler.transform(norm_tcolumns.values)\n",
    "training_examples = pd.DataFrame(scaledf, index=norm_tcolumns.index, columns=norm_tcolumns.columns) # scaledf is converted from array to dataframe\n",
    "x_train.update(training_examples)\n",
    "\n",
    "# validation\n",
    "norm_vcolumns=x_test[x_test.columns[:10]]\n",
    "vscaled = scaler.transform(norm_vcolumns.values) # this scaler uses std and mean of training dataset\n",
    "validation_examples = pd.DataFrame(vscaled, index=norm_vcolumns.index, columns=norm_vcolumns.columns)\n",
    "x_test.update(validation_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290506, 54)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "dimensionality = 54\n",
    "\n",
    "iter_num = 20 # number of iterations of alternating scheme\n",
    "steps_number = 10000 # number of gradient steps\n",
    "\n",
    "code_size = 20 #dimension of code\n",
    "k = 5 #needed dimension code_size = 2k k=10,20\n",
    "gamma = 10.0 # smoothness of manifold\n",
    "mu = 10.0 # main parameter mu=10,20,40,80,160\n",
    "epsilon = 0.1\n",
    "\n",
    "images = np.reshape(x_train, (-1, dimensionality))\n",
    "print(images.shape)\n",
    "labels = y_train\n",
    "\n",
    "test_images = np.reshape(x_test, (-1, dimensionality))\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "training_data = tf.placeholder(tf.float32, [None, dimensionality])\n",
    "gradient_training_data = tf.placeholder(tf.float32, [None, dimensionality])\n",
    "old_P = tf.placeholder(tf.float32, shape=[None, dimensionality, dimensionality])\n",
    "old_W = tf.placeholder(tf.float32, shape=[dimensionality, code_size])\n",
    "old_b = tf.placeholder(tf.float32, shape=[code_size])\n",
    "old_b_r = tf.placeholder(tf.float32, shape=[dimensionality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Variables to be tuned\n",
    "W = tf.Variable(tf.truncated_normal([dimensionality, code_size], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0.1, shape=[code_size]))\n",
    "\n",
    "b_r = tf.Variable(tf.constant(0.1, shape=[dimensionality]))\n",
    "\n",
    "code_data = tf.nn.sigmoid(tf.matmul(training_data, W) + b)\n",
    "recover = tf.matmul(code_data, tf.transpose(W)) + b_r\n",
    "grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        grad_phi_psi.append(tf.gradients(recover[i][j], [training_data[i]], unconnected_gradients='zero')[0])\n",
    "grad_phi_psi = tf.reshape(tf.stack(grad_phi_psi), [batch_size, dimensionality, dimensionality])\n",
    "\n",
    "\n",
    "# this is gradient field close to our points\n",
    "rand_training_data = training_data + tf.random.normal(shape=[batch_size, dimensionality],\n",
    "                                                      mean=0.0,stddev=epsilon)\n",
    "rand_code_data = tf.nn.sigmoid(tf.matmul(rand_training_data, W) + b)\n",
    "rand_recover = tf.matmul(rand_code_data, tf.transpose(W)) + b_r\n",
    "rand_grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        rand_grad_phi_psi.append(tf.gradients(rand_recover[i][j], [rand_training_data[i]], unconnected_gradients='zero')[0])\n",
    "rand_grad_phi_psi = tf.reshape(tf.stack(rand_grad_phi_psi), [batch_size, dimensionality, dimensionality])\n",
    "\n",
    "new_code_data = tf.nn.sigmoid(tf.matmul(gradient_training_data, W) + b)\n",
    "new_recover = tf.matmul(new_code_data, tf.transpose(W)) + b_r\n",
    "new_grad_phi_psi = []\n",
    "for i in range(batch_size):\n",
    "    for j in range(dimensionality):\n",
    "        new_grad_phi_psi.append(tf.gradients(new_recover[i][j], [gradient_training_data[i]], unconnected_gradients='zero')[0])\n",
    "new_grad_phi_psi = tf.reshape(tf.stack(new_grad_phi_psi), [batch_size, dimensionality, dimensionality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.square(training_data - recover)) + \\\n",
    "       gamma*tf.reduce_mean(tf.square(grad_phi_psi-rand_grad_phi_psi)) + \\\n",
    "       mu*tf.reduce_mean(tf.square(new_grad_phi_psi - old_P))\n",
    "\n",
    "# Training step\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training batch accuracy 24.7706 %\n",
      "Step 1000, training batch accuracy 18.1499 %\n",
      "Step 2000, training batch accuracy 11.252 %\n",
      "Step 3000, training batch accuracy 8.58562 %\n",
      "Step 4000, training batch accuracy 6.28406 %\n",
      "Step 5000, training batch accuracy 5.12293 %\n",
      "Step 6000, training batch accuracy 4.53011 %\n",
      "Step 7000, training batch accuracy 3.92896 %\n",
      "Step 8000, training batch accuracy 2.72213 %\n",
      "Step 9000, training batch accuracy 2.80199 %\n",
      "Step 0, training batch accuracy 2.44556 %\n",
      "Step 1000, training batch accuracy 2.1896 %\n",
      "Step 2000, training batch accuracy 1.74954 %\n",
      "Step 3000, training batch accuracy 1.87577 %\n",
      "Step 4000, training batch accuracy 1.63526 %\n",
      "Step 5000, training batch accuracy 1.60455 %\n",
      "Step 6000, training batch accuracy 1.7344 %\n",
      "Step 7000, training batch accuracy 1.98313 %\n",
      "Step 8000, training batch accuracy 1.48986 %\n",
      "Step 9000, training batch accuracy 1.70132 %\n",
      "Step 0, training batch accuracy 1.74349 %\n",
      "Step 1000, training batch accuracy 1.70317 %\n",
      "Step 2000, training batch accuracy 1.3994 %\n",
      "Step 3000, training batch accuracy 1.52154 %\n",
      "Step 4000, training batch accuracy 1.34775 %\n",
      "Step 5000, training batch accuracy 1.40668 %\n",
      "Step 6000, training batch accuracy 1.51711 %\n",
      "Step 7000, training batch accuracy 1.80695 %\n",
      "Step 8000, training batch accuracy 1.30149 %\n",
      "Step 9000, training batch accuracy 1.60521 %\n",
      "Step 0, training batch accuracy 1.62132 %\n",
      "Step 1000, training batch accuracy 1.55412 %\n",
      "Step 2000, training batch accuracy 1.29685 %\n",
      "Step 3000, training batch accuracy 1.38452 %\n",
      "Step 4000, training batch accuracy 1.23825 %\n",
      "Step 5000, training batch accuracy 1.31724 %\n",
      "Step 6000, training batch accuracy 1.42362 %\n",
      "Step 7000, training batch accuracy 1.74038 %\n",
      "Step 8000, training batch accuracy 1.21081 %\n",
      "Step 9000, training batch accuracy 1.55006 %\n",
      "Step 0, training batch accuracy 1.55394 %\n",
      "Step 1000, training batch accuracy 1.48317 %\n",
      "Step 2000, training batch accuracy 1.22722 %\n",
      "Step 3000, training batch accuracy 1.32018 %\n",
      "Step 4000, training batch accuracy 1.18694 %\n",
      "Step 5000, training batch accuracy 1.26222 %\n",
      "Step 6000, training batch accuracy 1.36347 %\n",
      "Step 7000, training batch accuracy 1.70769 %\n",
      "Step 8000, training batch accuracy 1.15871 %\n",
      "Step 9000, training batch accuracy 1.51174 %\n",
      "Step 0, training batch accuracy 1.50868 %\n",
      "Step 1000, training batch accuracy 1.44876 %\n",
      "Step 2000, training batch accuracy 1.19061 %\n",
      "Step 3000, training batch accuracy 1.28694 %\n",
      "Step 4000, training batch accuracy 1.16211 %\n",
      "Step 5000, training batch accuracy 1.22967 %\n",
      "Step 6000, training batch accuracy 1.33172 %\n",
      "Step 7000, training batch accuracy 1.69393 %\n",
      "Step 8000, training batch accuracy 1.12453 %\n",
      "Step 9000, training batch accuracy 1.491 %\n",
      "Step 0, training batch accuracy 1.47028 %\n",
      "Step 1000, training batch accuracy 1.42562 %\n",
      "Step 2000, training batch accuracy 1.17145 %\n",
      "Step 3000, training batch accuracy 1.2657 %\n",
      "Step 4000, training batch accuracy 1.14645 %\n",
      "Step 5000, training batch accuracy 1.20737 %\n",
      "Step 6000, training batch accuracy 1.31106 %\n",
      "Step 7000, training batch accuracy 1.68473 %\n",
      "Step 8000, training batch accuracy 1.10227 %\n",
      "Step 9000, training batch accuracy 1.47997 %\n",
      "Step 0, training batch accuracy 1.43369 %\n",
      "Step 1000, training batch accuracy 1.40759 %\n",
      "Step 2000, training batch accuracy 1.15983 %\n",
      "Step 3000, training batch accuracy 1.24974 %\n",
      "Step 4000, training batch accuracy 1.13408 %\n",
      "Step 5000, training batch accuracy 1.19249 %\n",
      "Step 6000, training batch accuracy 1.2965 %\n",
      "Step 7000, training batch accuracy 1.67486 %\n",
      "Step 8000, training batch accuracy 1.0889 %\n",
      "Step 9000, training batch accuracy 1.47409 %\n",
      "Step 0, training batch accuracy 1.40065 %\n",
      "Step 1000, training batch accuracy 1.39348 %\n",
      "Step 2000, training batch accuracy 1.15077 %\n",
      "Step 3000, training batch accuracy 1.23601 %\n",
      "Step 4000, training batch accuracy 1.12384 %\n",
      "Step 5000, training batch accuracy 1.18253 %\n",
      "Step 6000, training batch accuracy 1.28562 %\n",
      "Step 7000, training batch accuracy 1.66141 %\n",
      "Step 8000, training batch accuracy 1.08108 %\n",
      "Step 9000, training batch accuracy 1.47169 %\n",
      "Step 0, training batch accuracy 1.37352 %\n",
      "Step 1000, training batch accuracy 1.38323 %\n",
      "Step 2000, training batch accuracy 1.14296 %\n",
      "Step 3000, training batch accuracy 1.22366 %\n",
      "Step 4000, training batch accuracy 1.11565 %\n",
      "Step 5000, training batch accuracy 1.17588 %\n",
      "Step 6000, training batch accuracy 1.27757 %\n",
      "Step 7000, training batch accuracy 1.64597 %\n",
      "Step 8000, training batch accuracy 1.07641 %\n",
      "Step 9000, training batch accuracy 1.47176 %\n",
      "Step 0, training batch accuracy 1.35311 %\n",
      "Step 1000, training batch accuracy 1.37609 %\n",
      "Step 2000, training batch accuracy 1.13653 %\n",
      "Step 3000, training batch accuracy 1.21289 %\n",
      "Step 4000, training batch accuracy 1.10917 %\n",
      "Step 5000, training batch accuracy 1.17177 %\n",
      "Step 6000, training batch accuracy 1.27257 %\n",
      "Step 7000, training batch accuracy 1.63095 %\n",
      "Step 8000, training batch accuracy 1.07358 %\n",
      "Step 9000, training batch accuracy 1.47332 %\n",
      "Step 0, training batch accuracy 1.33831 %\n",
      "Step 1000, training batch accuracy 1.37114 %\n",
      "Step 2000, training batch accuracy 1.13175 %\n",
      "Step 3000, training batch accuracy 1.20385 %\n",
      "Step 4000, training batch accuracy 1.1039 %\n",
      "Step 5000, training batch accuracy 1.16918 %\n",
      "Step 6000, training batch accuracy 1.27007 %\n",
      "Step 7000, training batch accuracy 1.61733 %\n",
      "Step 8000, training batch accuracy 1.07187 %\n",
      "Step 9000, training batch accuracy 1.47533 %\n",
      "Step 0, training batch accuracy 1.32764 %\n",
      "Step 1000, training batch accuracy 1.36788 %\n",
      "Step 2000, training batch accuracy 1.12848 %\n",
      "Step 3000, training batch accuracy 1.19635 %\n",
      "Step 4000, training batch accuracy 1.09936 %\n",
      "Step 5000, training batch accuracy 1.16713 %\n",
      "Step 6000, training batch accuracy 1.2691 %\n",
      "Step 7000, training batch accuracy 1.60507 %\n",
      "Step 8000, training batch accuracy 1.07069 %\n",
      "Step 9000, training batch accuracy 1.47689 %\n",
      "Step 0, training batch accuracy 1.31987 %\n",
      "Step 1000, training batch accuracy 1.36585 %\n",
      "Step 2000, training batch accuracy 1.12613 %\n",
      "Step 3000, training batch accuracy 1.19 %\n",
      "Step 4000, training batch accuracy 1.09526 %\n",
      "Step 5000, training batch accuracy 1.16507 %\n",
      "Step 6000, training batch accuracy 1.26887 %\n",
      "Step 7000, training batch accuracy 1.59382 %\n",
      "Step 8000, training batch accuracy 1.06958 %\n",
      "Step 9000, training batch accuracy 1.47748 %\n",
      "Step 0, training batch accuracy 1.31408 %\n",
      "Step 1000, training batch accuracy 1.36449 %\n",
      "Step 2000, training batch accuracy 1.12417 %\n",
      "Step 3000, training batch accuracy 1.18443 %\n",
      "Step 4000, training batch accuracy 1.0915 %\n",
      "Step 5000, training batch accuracy 1.16281 %\n",
      "Step 6000, training batch accuracy 1.2689 %\n",
      "Step 7000, training batch accuracy 1.58323 %\n",
      "Step 8000, training batch accuracy 1.06822 %\n",
      "Step 9000, training batch accuracy 1.47695 %\n",
      "Step 0, training batch accuracy 1.30964 %\n",
      "Step 1000, training batch accuracy 1.3633 %\n",
      "Step 2000, training batch accuracy 1.12228 %\n",
      "Step 3000, training batch accuracy 1.1794 %\n",
      "Step 4000, training batch accuracy 1.08803 %\n",
      "Step 5000, training batch accuracy 1.16029 %\n",
      "Step 6000, training batch accuracy 1.26893 %\n",
      "Step 7000, training batch accuracy 1.57314 %\n",
      "Step 8000, training batch accuracy 1.06647 %\n",
      "Step 9000, training batch accuracy 1.47535 %\n",
      "Step 0, training batch accuracy 1.3061 %\n",
      "Step 1000, training batch accuracy 1.36195 %\n",
      "Step 2000, training batch accuracy 1.12034 %\n",
      "Step 3000, training batch accuracy 1.17478 %\n",
      "Step 4000, training batch accuracy 1.08484 %\n",
      "Step 5000, training batch accuracy 1.15751 %\n",
      "Step 6000, training batch accuracy 1.26886 %\n",
      "Step 7000, training batch accuracy 1.56353 %\n",
      "Step 8000, training batch accuracy 1.06425 %\n",
      "Step 9000, training batch accuracy 1.47278 %\n",
      "Step 0, training batch accuracy 1.30316 %\n",
      "Step 1000, training batch accuracy 1.36022 %\n",
      "Step 2000, training batch accuracy 1.11833 %\n",
      "Step 3000, training batch accuracy 1.17049 %\n",
      "Step 4000, training batch accuracy 1.08184 %\n",
      "Step 5000, training batch accuracy 1.15454 %\n",
      "Step 6000, training batch accuracy 1.26864 %\n",
      "Step 7000, training batch accuracy 1.55447 %\n",
      "Step 8000, training batch accuracy 1.06155 %\n",
      "Step 9000, training batch accuracy 1.46938 %\n",
      "Step 0, training batch accuracy 1.3006 %\n",
      "Step 1000, training batch accuracy 1.35804 %\n",
      "Step 2000, training batch accuracy 1.11631 %\n",
      "Step 3000, training batch accuracy 1.16646 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, training batch accuracy 1.07898 %\n",
      "Step 5000, training batch accuracy 1.15142 %\n",
      "Step 6000, training batch accuracy 1.26826 %\n",
      "Step 7000, training batch accuracy 1.54607 %\n",
      "Step 8000, training batch accuracy 1.05836 %\n",
      "Step 9000, training batch accuracy 1.46526 %\n",
      "Step 0, training batch accuracy 1.29825 %\n",
      "Step 1000, training batch accuracy 1.35541 %\n",
      "Step 2000, training batch accuracy 1.11431 %\n",
      "Step 3000, training batch accuracy 1.1626 %\n",
      "Step 4000, training batch accuracy 1.07618 %\n",
      "Step 5000, training batch accuracy 1.14817 %\n",
      "Step 6000, training batch accuracy 1.26763 %\n",
      "Step 7000, training batch accuracy 1.53837 %\n",
      "Step 8000, training batch accuracy 1.05469 %\n",
      "Step 9000, training batch accuracy 1.46053 %\n"
     ]
    }
   ],
   "source": [
    "N = 290506\n",
    "x_train = images[:N]\n",
    "N_grad = 1000\n",
    "grad_x_train = images[0:N_grad*290:290]\n",
    "\n",
    "\n",
    "cur_U = np.zeros((N_grad, dimensionality, k))\n",
    "cur_Sigma = np.zeros((N_grad, k, k))\n",
    "cur_V = np.zeros((N_grad, k, dimensionality))\n",
    "feed_P = np.zeros((batch_size, dimensionality, dimensionality))\n",
    "cur_W = np.random.normal(0, 0.35, (dimensionality, code_size))\n",
    "cur_b = np.zeros((code_size))\n",
    "cur_b_r = np.zeros((dimensionality))\n",
    "\n",
    "num_batches = int(N/batch_size)\n",
    "grad_num_batches = int(N_grad/batch_size)\n",
    "\n",
    "for iter in range(iter_num):\n",
    "    for i in range(steps_number):\n",
    "        # Get the next batch\n",
    "        which_batch = i%num_batches\n",
    "        input_batch = x_train[which_batch*batch_size:(which_batch+1)*batch_size]\n",
    "        grad_which_batch = i%grad_num_batches\n",
    "        grad_input_batch = grad_x_train[grad_which_batch*batch_size:(grad_which_batch+1)*batch_size]\n",
    "        for r in range(batch_size):\n",
    "            U = cur_U[grad_which_batch*batch_size+r]\n",
    "            Sigma = cur_Sigma[grad_which_batch*batch_size+r]\n",
    "            V = cur_V[grad_which_batch*batch_size+r]\n",
    "            feed_P[r] = np.matmul(U,np.matmul(Sigma,V))\n",
    "        feed_dict = {training_data: input_batch, gradient_training_data: grad_input_batch, \n",
    "                     old_P:feed_P,\n",
    "                  old_W:cur_W, old_b:cur_b, old_b_r:cur_b_r}\n",
    "        # Run the training step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "        # Print the accuracy progress on the batch every 1000 steps\n",
    "        if i%1000 == 0:\n",
    "            train_accuracy = sess.run(loss, feed_dict=feed_dict)\n",
    "            print(\"Step %d, training batch accuracy %g %%\"%(i, train_accuracy*100))\n",
    "    for grad_which_batch in range(grad_num_batches):\n",
    "        grad_input_batch = grad_x_train[grad_which_batch*batch_size:(grad_which_batch+1)*batch_size]\n",
    "        feed_dict = {gradient_training_data: grad_input_batch}\n",
    "        local_grad = sess.run(new_grad_phi_psi, feed_dict=feed_dict)\n",
    "        for r in range(batch_size):\n",
    "            u, s, vh = np.linalg.svd(local_grad[r,:,:], full_matrices=True)\n",
    "            cur_U[grad_which_batch*batch_size+r] = u[:,0:k:1]\n",
    "            cur_V[grad_which_batch*batch_size+r] = np.transpose(vh[:,0:k:1])\n",
    "            cur_Sigma[grad_which_batch*batch_size+r] = np.diag(s[0:k:1])\n",
    "    [cur_W, cur_b, cur_b_r] = sess.run([W, b, b_r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image[0] \tpred: 0 \torig: [1] \tacc: 0.0%\n",
      "test image[100] \tpred: 1 \torig: [1] \tacc: 71.29%\n",
      "test image[200] \tpred: 1 \torig: [1] \tacc: 74.13%\n",
      "test image[300] \tpred: 0 \torig: [0] \tacc: 72.76%\n",
      "test image[400] \tpred: 1 \torig: [0] \tacc: 73.57%\n",
      "test image[500] \tpred: 1 \torig: [0] \tacc: 72.85%\n",
      "test image[600] \tpred: 1 \torig: [1] \tacc: 74.21%\n",
      "test image[700] \tpred: 1 \torig: [1] \tacc: 73.47%\n",
      "test image[800] \tpred: 1 \torig: [1] \tacc: 74.03%\n",
      "test image[900] \tpred: 0 \torig: [0] \tacc: 73.47%\n",
      "test image[1000] \tpred: 1 \torig: [1] \tacc: 73.63%\n",
      "test image[1100] \tpred: 1 \torig: [1] \tacc: 73.3%\n",
      "test image[1200] \tpred: 0 \torig: [1] \tacc: 73.52%\n",
      "test image[1300] \tpred: 1 \torig: [1] \tacc: 73.48%\n",
      "test image[1400] \tpred: 1 \torig: [0] \tacc: 73.02%\n",
      "test image[1500] \tpred: 1 \torig: [1] \tacc: 72.68%\n",
      "test image[1600] \tpred: 5 \torig: [1] \tacc: 72.83%\n",
      "test image[1700] \tpred: 1 \torig: [4] \tacc: 73.07%\n",
      "test image[1800] \tpred: 1 \torig: [1] \tacc: 72.85%\n",
      "test image[1900] \tpred: 1 \torig: [0] \tacc: 72.7%\n",
      "test image[2000] \tpred: 0 \torig: [0] \tacc: 72.56%\n",
      "test image[2100] \tpred: 1 \torig: [1] \tacc: 72.49%\n",
      "test image[2200] \tpred: 0 \torig: [0] \tacc: 72.51%\n",
      "test image[2300] \tpred: 0 \torig: [0] \tacc: 72.49%\n",
      "test image[2400] \tpred: 2 \torig: [2] \tacc: 72.72%\n",
      "test image[2500] \tpred: 1 \torig: [1] \tacc: 72.81%\n",
      "test image[2600] \tpred: 1 \torig: [1] \tacc: 73.01%\n",
      "test image[2700] \tpred: 1 \torig: [1] \tacc: 73.2%\n",
      "test image[2800] \tpred: 1 \torig: [1] \tacc: 73.3%\n",
      "test image[2900] \tpred: 5 \torig: [1] \tacc: 73.08%\n",
      "test image[3000] \tpred: 1 \torig: [1] \tacc: 73.04%\n",
      "test image[3100] \tpred: 0 \torig: [0] \tacc: 73.01%\n",
      "test image[3200] \tpred: 2 \torig: [5] \tacc: 72.95%\n",
      "test image[3300] \tpred: 0 \torig: [0] \tacc: 73.13%\n",
      "test image[3400] \tpred: 1 \torig: [1] \tacc: 73.24%\n",
      "test image[3500] \tpred: 1 \torig: [1] \tacc: 73.26%\n",
      "test image[3600] \tpred: 1 \torig: [1] \tacc: 73.15%\n",
      "test image[3700] \tpred: 1 \torig: [1] \tacc: 73.06%\n",
      "test image[3800] \tpred: 1 \torig: [1] \tacc: 72.95%\n",
      "test image[3900] \tpred: 1 \torig: [1] \tacc: 73.08%\n",
      "test image[4000] \tpred: 1 \torig: [1] \tacc: 73.08%\n",
      "test image[4100] \tpred: 1 \torig: [1] \tacc: 73.13%\n",
      "test image[4200] \tpred: 1 \torig: [0] \tacc: 73.2%\n",
      "test image[4300] \tpred: 2 \torig: [1] \tacc: 72.94%\n",
      "test image[4400] \tpred: 1 \torig: [0] \tacc: 72.89%\n",
      "test image[4500] \tpred: 1 \torig: [1] \tacc: 72.96%\n",
      "test image[4600] \tpred: 1 \torig: [1] \tacc: 72.98%\n",
      "test image[4700] \tpred: 1 \torig: [1] \tacc: 73.01%\n",
      "test image[4800] \tpred: 1 \torig: [1] \tacc: 72.78%\n",
      "test image[4900] \tpred: 0 \torig: [0] \tacc: 73.03%\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(img_a, img_b):\n",
    "    '''Finds the distance between 2 images: img_a, img_b'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((img_a - img_b) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label[0]] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "train_images = np.asarray(images[:5000])\n",
    "train_labels = np.asarray(labels[:5000])\n",
    "test_images = np.asarray(test_images[:5000])\n",
    "test_labels = np.asarray(test_labels[:5000])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. /(1+np.exp(-x))\n",
    "def new_euclidean_distance(img_a, img_b):\n",
    "    img_a = np.reshape(img_a, (1,-1))\n",
    "    img_b = np.reshape(img_b, (1,-1))\n",
    "    img_a = sigmoid(np.matmul(img_a, cur_W) + cur_b)\n",
    "    img_b = sigmoid(np.matmul(img_b, cur_W) + cur_b)\n",
    "    return np.sum((img_a - img_b) ** 2)\n",
    "\n",
    "def new_predict(k, train_images, train_labels, test_images):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(new_euclidean_distance(test_image, image), label)\n",
    "                    for (image, label) in zip(train_images, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "# Predicting and printing the accuracy\n",
    "i = 0\n",
    "total_correct = 0\n",
    "for test_image in test_images[:5000]:\n",
    "    pred = new_predict(10, train_images, train_labels, test_image)\n",
    "    if pred == test_labels[i]:\n",
    "        total_correct += 1\n",
    "    acc = (total_correct / (i+1)) * 100\n",
    "    if i%100 == 0:\n",
    "        print('test image['+str(i)+']', '\\tpred:', pred, '\\torig:', test_labels[i], '\\tacc:', str(round(acc, 2))+'%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image[0] \tpred: 0 \torig: [1] \tacc: 0.0%\n",
      "test image[100] \tpred: 1 \torig: [1] \tacc: 75.25%\n",
      "test image[200] \tpred: 1 \torig: [1] \tacc: 71.14%\n",
      "test image[300] \tpred: 0 \torig: [0] \tacc: 69.44%\n",
      "test image[400] \tpred: 1 \torig: [0] \tacc: 71.82%\n",
      "test image[500] \tpred: 1 \torig: [0] \tacc: 72.26%\n",
      "test image[600] \tpred: 1 \torig: [1] \tacc: 72.88%\n",
      "test image[700] \tpred: 1 \torig: [1] \tacc: 72.75%\n",
      "test image[800] \tpred: 0 \torig: [1] \tacc: 73.53%\n",
      "test image[900] \tpred: 0 \torig: [0] \tacc: 73.47%\n",
      "test image[1000] \tpred: 1 \torig: [1] \tacc: 74.23%\n",
      "test image[1100] \tpred: 1 \torig: [1] \tacc: 73.84%\n",
      "test image[1200] \tpred: 1 \torig: [1] \tacc: 74.1%\n",
      "test image[1300] \tpred: 1 \torig: [1] \tacc: 73.79%\n",
      "test image[1400] \tpred: 0 \torig: [0] \tacc: 73.45%\n",
      "test image[1500] \tpred: 1 \torig: [1] \tacc: 72.88%\n",
      "test image[1600] \tpred: 5 \torig: [1] \tacc: 73.08%\n",
      "test image[1700] \tpred: 1 \torig: [4] \tacc: 73.25%\n",
      "test image[1800] \tpred: 1 \torig: [1] \tacc: 73.51%\n",
      "test image[1900] \tpred: 0 \torig: [0] \tacc: 73.59%\n",
      "test image[2000] \tpred: 0 \torig: [0] \tacc: 73.46%\n",
      "test image[2100] \tpred: 1 \torig: [1] \tacc: 73.35%\n",
      "test image[2200] \tpred: 0 \torig: [0] \tacc: 73.28%\n",
      "test image[2300] \tpred: 6 \torig: [0] \tacc: 73.23%\n",
      "test image[2400] \tpred: 2 \torig: [2] \tacc: 73.09%\n",
      "test image[2500] \tpred: 1 \torig: [1] \tacc: 73.17%\n",
      "test image[2600] \tpred: 1 \torig: [1] \tacc: 73.36%\n",
      "test image[2700] \tpred: 1 \torig: [1] \tacc: 73.53%\n",
      "test image[2800] \tpred: 0 \torig: [1] \tacc: 73.51%\n",
      "test image[2900] \tpred: 5 \torig: [1] \tacc: 73.39%\n",
      "test image[3000] \tpred: 1 \torig: [1] \tacc: 73.44%\n",
      "test image[3100] \tpred: 0 \torig: [0] \tacc: 73.52%\n",
      "test image[3200] \tpred: 2 \torig: [5] \tacc: 73.6%\n",
      "test image[3300] \tpred: 0 \torig: [0] \tacc: 73.74%\n",
      "test image[3400] \tpred: 1 \torig: [1] \tacc: 73.89%\n",
      "test image[3500] \tpred: 1 \torig: [1] \tacc: 73.92%\n",
      "test image[3600] \tpred: 1 \torig: [1] \tacc: 73.81%\n",
      "test image[3700] \tpred: 1 \torig: [1] \tacc: 73.76%\n",
      "test image[3800] \tpred: 0 \torig: [1] \tacc: 73.66%\n",
      "test image[3900] \tpred: 1 \torig: [1] \tacc: 73.78%\n",
      "test image[4000] \tpred: 1 \torig: [1] \tacc: 73.81%\n",
      "test image[4100] \tpred: 1 \torig: [1] \tacc: 73.81%\n",
      "test image[4200] \tpred: 1 \torig: [0] \tacc: 73.89%\n",
      "test image[4300] \tpred: 1 \torig: [1] \tacc: 73.73%\n",
      "test image[4400] \tpred: 1 \torig: [0] \tacc: 73.76%\n",
      "test image[4500] \tpred: 1 \torig: [1] \tacc: 73.76%\n",
      "test image[4600] \tpred: 0 \torig: [1] \tacc: 73.77%\n",
      "test image[4700] \tpred: 1 \torig: [1] \tacc: 73.81%\n",
      "test image[4800] \tpred: 0 \torig: [1] \tacc: 73.61%\n",
      "test image[4900] \tpred: 0 \torig: [0] \tacc: 73.78%\n"
     ]
    }
   ],
   "source": [
    "def predict(k, train_images, train_labels, test_images):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_image, image), label)\n",
    "                    for (image, label) in zip(train_images, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "# Predicting and printing the accuracy\n",
    "i = 0\n",
    "total_correct = 0\n",
    "for test_image in test_images[:5000]:\n",
    "    pred = predict(10, train_images, train_labels, test_image)\n",
    "    if pred == test_labels[i]:\n",
    "        total_correct += 1\n",
    "    acc = (total_correct / (i+1)) * 100\n",
    "    if i%100 == 0:\n",
    "        print('test image['+str(i)+']', '\\tpred:', pred, '\\torig:', test_labels[i], '\\tacc:', str(round(acc, 2))+'%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
